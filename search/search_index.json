{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Selvie Akmalia 160411100051 Penambangan dan Pencarian Web - Dosen Pengampu Bapak Mula'ab, S.si, M.kom Teknik Informatika Universitas Trunojoyo Madura Pengantar \u00b6 Apa itu Web Crawler ? Web Crawler adalah sebuah program dengan metode tertentu yang berfungsi untuk melakukan Scan atau crawl ke semua halaman internet untuk mencari data yang diinginkan. Data tersebut merupakan hasil teratas dari mesin pencari google dan yahoo. Web Crawler atau web spider, web robot, bot crawl dan automathic indeker. Web clawer memiliki beberapa maafaat dan tujuan tetapi penggunaan yang paling umum digunakan sebagai search engine. Search engine merupakan sebuah tool yang berfungsi sebagai mesin pencari seperti google. Kelebihan Web Crawler Kelebihan Suatu program atau script yang relatif simple Proses sebuah web crawler untuk mendata link \u2013 link yang terdapat didalam sebuah halaman web menggunakan pendekatan regular expression. Crawler akan menelurusi setiap karakter yang ada untuk menemukan hyperlink tag html. Setiap hyperlink tag yang ditemukan diperiksa lebih lanjut apakah tag tersebut mengandung atribut nofollow rel, jika tidak ada maka diambil nilai yang terdapat didalam attribute href yang merupakan sebuah link baru. Dapat digunakan untuk beragam tujuan. Penggunaan yang paling umum adalah yang terkait atau berhubungan langsung dengan search engine. Kekurangan Mengingat dia sejatinya mesin atau program, kita mesti menyesuaikan (semisal konten yang berguna) agar web crawler juga tahu bahwa halaman yang sedang dijelajahinya berisi informasi bermanfaat. Sebab, mesin web crawlerc menandai proses-proses tertentu dalam situs sebagai kelemahan. Misalnya, halaman-halaman yang lamban saat dipanggil mungkin dilewati begitu saja. Atau, lantaran konfigurasi situs tidak terlalu bagus membuat si web crawler kurang efisien kala mengeksplorasi. Lalu, dia lebih fokus pada beberapa halaman dan melewatkan yang lain padahal mungkin isinya penting. Ini akan menjadi tantangan tersendiri bagi situs ritel besar yang banyak mengandung konten dinamis, yaitu halaman web yang kompleks (misalnya konten yang dipersonalisasi). Tools & Environment \u00b6 Menggunakan bahasa pemrograman Python versi 3.6 Library yang digunakan untuk crawling : requests BeautifulSoup4 (bs4) Library untuk menyimpan data : sqlite (database) csv (import data ke csv) Library yang digunakan untuk preprocessing : Sastrawi Library yang digunakan untuk seleksi fitur & clustering : scipy numpy sklearn (Scikit-learn) skfuzzy (Scikit-fuzzy) Website target : https://www.anehdidunia.com/ Running program membutuhkan koneksi internet untuk mengambil data dari website target","title":"Home"},{"location":"#pengantar","text":"Apa itu Web Crawler ? Web Crawler adalah sebuah program dengan metode tertentu yang berfungsi untuk melakukan Scan atau crawl ke semua halaman internet untuk mencari data yang diinginkan. Data tersebut merupakan hasil teratas dari mesin pencari google dan yahoo. Web Crawler atau web spider, web robot, bot crawl dan automathic indeker. Web clawer memiliki beberapa maafaat dan tujuan tetapi penggunaan yang paling umum digunakan sebagai search engine. Search engine merupakan sebuah tool yang berfungsi sebagai mesin pencari seperti google. Kelebihan Web Crawler Kelebihan Suatu program atau script yang relatif simple Proses sebuah web crawler untuk mendata link \u2013 link yang terdapat didalam sebuah halaman web menggunakan pendekatan regular expression. Crawler akan menelurusi setiap karakter yang ada untuk menemukan hyperlink tag html. Setiap hyperlink tag yang ditemukan diperiksa lebih lanjut apakah tag tersebut mengandung atribut nofollow rel, jika tidak ada maka diambil nilai yang terdapat didalam attribute href yang merupakan sebuah link baru. Dapat digunakan untuk beragam tujuan. Penggunaan yang paling umum adalah yang terkait atau berhubungan langsung dengan search engine. Kekurangan Mengingat dia sejatinya mesin atau program, kita mesti menyesuaikan (semisal konten yang berguna) agar web crawler juga tahu bahwa halaman yang sedang dijelajahinya berisi informasi bermanfaat. Sebab, mesin web crawlerc menandai proses-proses tertentu dalam situs sebagai kelemahan. Misalnya, halaman-halaman yang lamban saat dipanggil mungkin dilewati begitu saja. Atau, lantaran konfigurasi situs tidak terlalu bagus membuat si web crawler kurang efisien kala mengeksplorasi. Lalu, dia lebih fokus pada beberapa halaman dan melewatkan yang lain padahal mungkin isinya penting. Ini akan menjadi tantangan tersendiri bagi situs ritel besar yang banyak mengandung konten dinamis, yaitu halaman web yang kompleks (misalnya konten yang dipersonalisasi).","title":"Pengantar"},{"location":"#tools-environment","text":"Menggunakan bahasa pemrograman Python versi 3.6 Library yang digunakan untuk crawling : requests BeautifulSoup4 (bs4) Library untuk menyimpan data : sqlite (database) csv (import data ke csv) Library yang digunakan untuk preprocessing : Sastrawi Library yang digunakan untuk seleksi fitur & clustering : scipy numpy sklearn (Scikit-learn) skfuzzy (Scikit-fuzzy) Website target : https://www.anehdidunia.com/ Running program membutuhkan koneksi internet untuk mengambil data dari website target","title":"Tools &amp; Environment"},{"location":"Clustering/","text":"Clustering \u00b6 Tahapan selanjutnya adalah Clustering. Sama seperti sebelumya yaitu saya menggunakan bahasa pemrograman Python versi 3.6. Apa itu clustering ? Clustering adalah metode penganalisaan data, yang sering dimasukkan sebagai salah satu metode Data Mining , yang tujuannya adalah untuk mengelompokkan data dengan karakteristik yang sama ke suatu \u2018wilayah\u2019 yang sama dan data dengan karakteristik yang berbeda ke \u2018wilayah\u2019 yang lain. Clustering Dengan Pendekatan Partisi : K-Means \u00b6 Salah satu metode yang banyak digunakan dalam melakukan clustering dengan partisi ini adalah metode k-means. Secara umum metode k-means ini melakukan proses pengelompokan dengan prosedur sebagai berikut: Tentukan jumlah cluster Alokasikan data secara random ke cluster yang ada Hitung rata-rata setiap cluster dari data yang tergabung di dalamnya Alokasikan kembali semua data ke cluster terdekat Ulang proses nomor 3, sampai tidak ada perubahan atau perubahan yang terjadi masih sudah di bawah treshold Prosedur dasar ini bisa berubah mengikuti pendekatan pengalokasian data yang diterapkan, apakah crisp atau fuzzy . Setelah meneliti clustering dari sudut yang lain, saya menemukan bahwa k-means clustering mempunyai beberapa kelemahan. Mixture Modelling (Mixture Modeling) Mixture modelling (mixture modeling) merupakan metode pengelompokan data yang mirip dengan k-means dengan kelebihan penggunaan distribusi statistik dalam mendefinisikan setiap cluster yang ditemukan. Dibandingkan dengan k-means yang hanya menggunakan cluster center, penggunaan distribusi statistik ini mengijinkan kita untuk: Memodel data yang kita miliki dengan setting karakteristik yang berbeda-beda Jumlah cluster yang sesuai dengan keadaan data bisa ditemukan seiring dengan proses pemodelan karakteristik dari masing-masing cluster Hasil pemodelan clustering yang dilaksanakan bisa diuji tingkat keakuratannya Distribusi statistik yang digunakan bisa bermacam-macam mulai dari yang digunakan untuk data categorical sampai yang continuous, termasuk di antaranya distribusi binomial, multinomial, normal dan lain-lain. Beberapa distribusi yang bersifat tidak normal seperti distribusi Poisson, von-Mises, Gamma dan Student t, juga diterapkan untuk bisa mengakomodasi berbagai keadaan data yang ada di lapangan. Beberapa pendekatan multivariate juga banyak diterapkan untuk memperhitungkan tingkat keterkaitan antara variabel data yang satu dengan yang lainnya. Data clustering menggunakan metode K-Means Clustering ini secara umum dilakukan dengan algoritma dasar sebagai berikut: Tentukan jumlah cluster Alokasikan data ke dalam cluster secara random Hitung centroid/rata-rata dari data yang ada di masing-masing cluster Alokasikan masing-masing data ke centroid/rata-rata terdekat Kembali ke Step 3, apabila masih ada data yang berpindah cluster atau apabila perubahan nilai centroid, ada yang di atas nilai threshold yang ditentukan atau apabila perubahan nilai pada objective function yang digunakan di atas nilai threshold yang ditentukan Code dibawah ini digunakan untuk melakukan clustering, sebagai contoh, cluster dibagi menjadi 5. Namun, nantinya cluster dapat diubah sesuai dengan kebutuhan. # Clustering kmeans = KMeans(n_clusters=5, random_state=0).fit(fiturBaru) write_csv(\"Kluster_label.csv\", [kmeans.labels_]) s_avg = silhouette_score(fiturBaru, kmeans.labels_, random_state=10) print(s_avg) for i in range(len(kmeans.labels_)): print(\"Doc %d =>> cluster %d\" %(i+1, kmeans.labels_[i])) Fuzzy K-Means \u00b6 Metode Fuzzy K-Means (atau lebih sering disebut sebagai Fuzzy C-Means ) mengalokasikan kembali data ke dalam masing-masing cluster dengan memanfaatkan teori Fuzzy. Teori ini mengeneralisasikan metode pengalokasian yang bersifat tegas (hard) seperti yang digunakan pada metode Hard K-Means. Dalam metode Fuzzy K-Means dipergunakan variabel membership function, uik, yang merujuk pada seberapa besar kemungkinan suatu data bisa menjadi anggota ke dalam suatu cluster. Konsep dari Fuzzy C-Means pertama kali adalah menentukan pusat cluster, yang akan menandai lokasi rata-rata untuk tiap-tiap cluster. Pada kondisi awal, pusat cluster ini masih belum akurat. Tiap-tiap titik data memiliki derajat keanggotaan untuk tiap-tiap cluster. Dengan cara memperbaiki pusat cluster dan derajat keanggotaan tiap-tiap titik data secara berulang, maka akan dapat dilihat bahwa pusat cluster akan bergerak menuju lokasi yang tepat. Perulangan ini didasarkan pada minimasi fungsi obyektif yang menggambarkan jarak dari titik data yang diberikan kepusat cluster yang terbobot oleh derajat keanggotaan titik data tersebut. Output dari Fuzzy C-Means merupakan deretan usat cluster dan beberapa derajat keanggotaan untuk tiap-tiap titik data. Informasi ini dapat digunakan untuk membangun suatu fuzzy inference system. Algoritma Fuzzy K-Means adalah sebagai berikut : Shilhoutte Coefficient** \u00b6 Metode Shilhoutte Coefficient ini berfungsi untuk menguji kualitas dari cluster yang dihasilkan. Metode ini merupakan metode validasi cluster yang menggabungkan metode cohesion dan Separation . Untuk menghitung nilai silhoutte coefisient diperlukan jarak antar dokumen dengan menggunakan rumus EuclideanDistance . Setelah itu tahapan untuk menghitung nilai silhoutte coeffisien adalah sebagai berikut : Untuk setiap objek i, hitung rata-rata jarak dari objek i dengan seluruh objek yang berada dalam satu cluster. Akan didapatkan nilai rata-rata yang disebut a*i*. Untuk setiap objek i, hitung rata-rata jarak dari objek i dengan objek yang berada di cluster lainnya. Dari semua jarak rata-rata tersebut ambil nilai yang paling kecil. Nilai ini disebut b*i*. Setelah itu maka untuk objek i memiliki nilai silhoutte coefisien : S*i*= (b*i* \u2013 a*i*) / max(a*i*, b*i*) Hasil perhitungan nilai silhoutte coeffisien dapat bervariasi antara -1 hingga 1. Hasil clustering dikatakan baik jikai nilai silhoutte coeffisien bernilai positif (a*i* < b*i*) dan a*i* mendekati 0, sehingga akan menghasilkan nilai silhoutte coeffisien yang maksimum yaitu 1 saat a*i* = 0. Maka dapat dikatakan, jika s*i* = 1 berarti objek i sudah berada dalam cluster yang tepat. Jika nilai s*i* = 0 maka objek i*berada di antara dua cluster sehingga objek tersebut tidak jelas harus dimasukan ke dalam cluster A atau cluster B. Akan tetapi, jika s*i = -1 artinya struktur cluster yang dihasilkan overlapping, sehingga objek i lebih tepat dimasukan ke dalam cluster yang lain. Nilai rata-rata silhoutte coeffisien dari tiap objek dalam suatu cluster adalah suatu ukuran yang menunjukan seberapa ketat data dikelompokan dalam cluster tersebut. Berikut adalah nilai silhoutte berdasarkan Kaufman dan Rousseeuw : 0.7 < SC <= 1 Strong Stucture 0.5 < SC <= 0.7 Medium Structure 0.25 < SC <= 0.5 Weak Structure SC <= 0.25 No structure Code untuk shilhoutte coefficient : s_avg = silhouette_score(fiturBaru, kmeans.labels_, random_state=10)","title":"Clustering"},{"location":"Clustering/#clustering","text":"Tahapan selanjutnya adalah Clustering. Sama seperti sebelumya yaitu saya menggunakan bahasa pemrograman Python versi 3.6. Apa itu clustering ? Clustering adalah metode penganalisaan data, yang sering dimasukkan sebagai salah satu metode Data Mining , yang tujuannya adalah untuk mengelompokkan data dengan karakteristik yang sama ke suatu \u2018wilayah\u2019 yang sama dan data dengan karakteristik yang berbeda ke \u2018wilayah\u2019 yang lain. Clustering Dengan Pendekatan Partisi :","title":"Clustering"},{"location":"Clustering/#k-means","text":"Salah satu metode yang banyak digunakan dalam melakukan clustering dengan partisi ini adalah metode k-means. Secara umum metode k-means ini melakukan proses pengelompokan dengan prosedur sebagai berikut: Tentukan jumlah cluster Alokasikan data secara random ke cluster yang ada Hitung rata-rata setiap cluster dari data yang tergabung di dalamnya Alokasikan kembali semua data ke cluster terdekat Ulang proses nomor 3, sampai tidak ada perubahan atau perubahan yang terjadi masih sudah di bawah treshold Prosedur dasar ini bisa berubah mengikuti pendekatan pengalokasian data yang diterapkan, apakah crisp atau fuzzy . Setelah meneliti clustering dari sudut yang lain, saya menemukan bahwa k-means clustering mempunyai beberapa kelemahan. Mixture Modelling (Mixture Modeling) Mixture modelling (mixture modeling) merupakan metode pengelompokan data yang mirip dengan k-means dengan kelebihan penggunaan distribusi statistik dalam mendefinisikan setiap cluster yang ditemukan. Dibandingkan dengan k-means yang hanya menggunakan cluster center, penggunaan distribusi statistik ini mengijinkan kita untuk: Memodel data yang kita miliki dengan setting karakteristik yang berbeda-beda Jumlah cluster yang sesuai dengan keadaan data bisa ditemukan seiring dengan proses pemodelan karakteristik dari masing-masing cluster Hasil pemodelan clustering yang dilaksanakan bisa diuji tingkat keakuratannya Distribusi statistik yang digunakan bisa bermacam-macam mulai dari yang digunakan untuk data categorical sampai yang continuous, termasuk di antaranya distribusi binomial, multinomial, normal dan lain-lain. Beberapa distribusi yang bersifat tidak normal seperti distribusi Poisson, von-Mises, Gamma dan Student t, juga diterapkan untuk bisa mengakomodasi berbagai keadaan data yang ada di lapangan. Beberapa pendekatan multivariate juga banyak diterapkan untuk memperhitungkan tingkat keterkaitan antara variabel data yang satu dengan yang lainnya. Data clustering menggunakan metode K-Means Clustering ini secara umum dilakukan dengan algoritma dasar sebagai berikut: Tentukan jumlah cluster Alokasikan data ke dalam cluster secara random Hitung centroid/rata-rata dari data yang ada di masing-masing cluster Alokasikan masing-masing data ke centroid/rata-rata terdekat Kembali ke Step 3, apabila masih ada data yang berpindah cluster atau apabila perubahan nilai centroid, ada yang di atas nilai threshold yang ditentukan atau apabila perubahan nilai pada objective function yang digunakan di atas nilai threshold yang ditentukan Code dibawah ini digunakan untuk melakukan clustering, sebagai contoh, cluster dibagi menjadi 5. Namun, nantinya cluster dapat diubah sesuai dengan kebutuhan. # Clustering kmeans = KMeans(n_clusters=5, random_state=0).fit(fiturBaru) write_csv(\"Kluster_label.csv\", [kmeans.labels_]) s_avg = silhouette_score(fiturBaru, kmeans.labels_, random_state=10) print(s_avg) for i in range(len(kmeans.labels_)): print(\"Doc %d =>> cluster %d\" %(i+1, kmeans.labels_[i]))","title":"K-Means"},{"location":"Clustering/#fuzzy-k-means","text":"Metode Fuzzy K-Means (atau lebih sering disebut sebagai Fuzzy C-Means ) mengalokasikan kembali data ke dalam masing-masing cluster dengan memanfaatkan teori Fuzzy. Teori ini mengeneralisasikan metode pengalokasian yang bersifat tegas (hard) seperti yang digunakan pada metode Hard K-Means. Dalam metode Fuzzy K-Means dipergunakan variabel membership function, uik, yang merujuk pada seberapa besar kemungkinan suatu data bisa menjadi anggota ke dalam suatu cluster. Konsep dari Fuzzy C-Means pertama kali adalah menentukan pusat cluster, yang akan menandai lokasi rata-rata untuk tiap-tiap cluster. Pada kondisi awal, pusat cluster ini masih belum akurat. Tiap-tiap titik data memiliki derajat keanggotaan untuk tiap-tiap cluster. Dengan cara memperbaiki pusat cluster dan derajat keanggotaan tiap-tiap titik data secara berulang, maka akan dapat dilihat bahwa pusat cluster akan bergerak menuju lokasi yang tepat. Perulangan ini didasarkan pada minimasi fungsi obyektif yang menggambarkan jarak dari titik data yang diberikan kepusat cluster yang terbobot oleh derajat keanggotaan titik data tersebut. Output dari Fuzzy C-Means merupakan deretan usat cluster dan beberapa derajat keanggotaan untuk tiap-tiap titik data. Informasi ini dapat digunakan untuk membangun suatu fuzzy inference system. Algoritma Fuzzy K-Means adalah sebagai berikut :","title":"Fuzzy K-Means"},{"location":"Clustering/#shilhoutte-coefficient","text":"Metode Shilhoutte Coefficient ini berfungsi untuk menguji kualitas dari cluster yang dihasilkan. Metode ini merupakan metode validasi cluster yang menggabungkan metode cohesion dan Separation . Untuk menghitung nilai silhoutte coefisient diperlukan jarak antar dokumen dengan menggunakan rumus EuclideanDistance . Setelah itu tahapan untuk menghitung nilai silhoutte coeffisien adalah sebagai berikut : Untuk setiap objek i, hitung rata-rata jarak dari objek i dengan seluruh objek yang berada dalam satu cluster. Akan didapatkan nilai rata-rata yang disebut a*i*. Untuk setiap objek i, hitung rata-rata jarak dari objek i dengan objek yang berada di cluster lainnya. Dari semua jarak rata-rata tersebut ambil nilai yang paling kecil. Nilai ini disebut b*i*. Setelah itu maka untuk objek i memiliki nilai silhoutte coefisien : S*i*= (b*i* \u2013 a*i*) / max(a*i*, b*i*) Hasil perhitungan nilai silhoutte coeffisien dapat bervariasi antara -1 hingga 1. Hasil clustering dikatakan baik jikai nilai silhoutte coeffisien bernilai positif (a*i* < b*i*) dan a*i* mendekati 0, sehingga akan menghasilkan nilai silhoutte coeffisien yang maksimum yaitu 1 saat a*i* = 0. Maka dapat dikatakan, jika s*i* = 1 berarti objek i sudah berada dalam cluster yang tepat. Jika nilai s*i* = 0 maka objek i*berada di antara dua cluster sehingga objek tersebut tidak jelas harus dimasukan ke dalam cluster A atau cluster B. Akan tetapi, jika s*i = -1 artinya struktur cluster yang dihasilkan overlapping, sehingga objek i lebih tepat dimasukan ke dalam cluster yang lain. Nilai rata-rata silhoutte coeffisien dari tiap objek dalam suatu cluster adalah suatu ukuran yang menunjukan seberapa ketat data dikelompokan dalam cluster tersebut. Berikut adalah nilai silhoutte berdasarkan Kaufman dan Rousseeuw : 0.7 < SC <= 1 Strong Stucture 0.5 < SC <= 0.7 Medium Structure 0.25 < SC <= 0.5 Weak Structure SC <= 0.25 No structure Code untuk shilhoutte coefficient : s_avg = silhouette_score(fiturBaru, kmeans.labels_, random_state=10)","title":"Shilhoutte Coefficient**"},{"location":"Ekstraksi_Teks/","text":"Ekstraksi Teks \u00b6 Tahapan selanjutnya adalah Ekstraksi Teks. Sama seperti sebelumya yaitu saya menggunakan bahasa pemrograman Python versi 3.6. Tahapan - tahapan dalam ekstraksi teks yaitu : Tahap Stop word Removal Stop word adalah kata umum (common words) yang biasanya muncul dalam jumlah besar dan dianggap tidak memiliki makna. Stop word umumnya dimanfaatkan dalam task information retrieval. Contoh stop word untuk bahasa Inggris diantaranya \u201cof\u201d, \u201cthe\u201d. Sedangkan untuk bahasa Indonesia diantaranya \u201cyang\u201d, \u201cdi\u201d, \u201cke\u201d. Tahap Stemming Stemming merupakan pemotongan suatu kata menjadi kata dasar, jadi jika terdapat kata imbuhan, maka imbuhan akan dihilangkan, dan hanya diambil kata dasarnya saja. Contohnya kata \"tersaring\" menjadi \"saring\". Agar dapat melakukan kedua tahapan diatas, maka perlu menginstall library Sastrawi dan Sklearn. Caranya adalah, pertama - tama membuka command prompt di laptop anda : Ketikkan kode berikut : pip install Sastrawi pip install Sastrawi digunakan untuk menginstall Sastrawi. pip install Sklearn pip install Sklearn digunakan untuk menginstall Sklearn Setelah Sastrawi dan Sklearn telah terinstall, kalian bisa membuka Python, lalu Ketikkan code dibawah ini : from Sastrawi.Stemmer.StemmerFactory import StemmerFactory from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory # For processing purpose from sklearn.feature_extraction.text import CountVectorizer from sklearn.feature_extraction.text import TfidfVectorizer Code diatas digunakan untuk mengambil semua library yang telah terinstall Setelah mengimport seluruh library yang dibutuhkan, maka langkah selanjutnya adalah ketikkan code dibawah ini : #VSM def countWord(txt): ''' Fungsi ini digunakan untuk menghitung setiap kata pada satu string ''' d = dict() for i in txt.split(): if d.get(i) == None: d[i] = txt.count(i) return d def add_row_VSM(d): ''' Fungsi ini digunakan untuk membangun VSM ''' #init baris baru VSM.append([]) # memasukkan kata berdasarkan kata yang telah ditemukan sebelumnya for i in VSM[0]: if d.get(i) == None: VSM[-1].append(0) else : VSM[-1].append(d.pop(i)); # memasukkan kata baru for i in d: VSM[0].append(i) for j in range(1, len(VSM)-1): VSM[j].insert(-2,0) VSM[-1].append(d.get(i)) Fungsi dari countword untuk menghitung banyaknya kata pada setiap dokumen, sedangkan fungsi add_row_vsm adalah membuat sebuah matrix vsm. Contoh dokumen untuk penghitungan kata pada setiap dokumen adalah sebagai berikut : No dahulu kala ada seekor kelinci dan tikus 1 1 1 3 3 2 1 1 2 0 0 2 1 2 1 0 3 0 1 1 1 1 1 1 4 1 1 0 2 1 2 2 Selanjutnya adalah mengetikkan code dibawah ini : cursor = conn.execute(\"SELECT * from Berita\") cursor = cursor.fetchall() #cursor = cursor[:10] pertama = True corpus = list() c=1 for row in cursor: print ('Proses : %.2f' %((c/len(cursor))*100) + '%'); c+=1 txt = row[0] cleaned = preprosesing(txt) corpus.append(cleaned) ''' d = countWord(cleaned) if pertama: pertama = False VSM = list((list(), list())) for key in d: VSM[0].append(key) VSM[1].append(d[key]) else: add_row_VSM(d) ''' Code diatas berguna untuk pemanggilan fungsi - fungsi pada code yang terdapat fungsi countword nya. Lalu, menampilkan hasilnya dengan di ekspor ke csv agar nantinya lebih mudah untuk dibaca. Caranya adalah dengan mengetikkan code dibawah ini : def write_csv(nama_file, isi, tipe='w'): 'tipe=w; write; tipe=a; append;' with open(nama_file, mode=tipe) as tbl: tbl_writer = csv.writer(tbl, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) for row in isi: tbl_writer.writerow(row) Berikut merupakan penjelasan TFIDF : Perhitungan TFIDF \u00b6 Menghitung Term Frequency (tf). Term frequency (tf) merupakan frekuensi kemunculan term (t) pada dokumen (d). Contoh: Terdapat kalimat: Saya sedang belajar menghitung tf.idf. Tf.idf merupakan frekuensi kemunculan term pada dokumen. Langkah awal perhitungan tersebut adalah menghitung tf, kemudian menghitung df dan idf. Langkah terakhir menghitung nilai tf.idf. Mari kita belajar! Catatan: tiap kalimat dianggap sebagai dokumen. Tentukan nilai tf! Jawaban: Jadi dokumen tiap kalimat ditandai: Saya sedang belajar menghitung tf.idf. Tf.idf merupakan frekuensi kemunculan term pada dokumen. Langkah awal perhitungan tersebut adalah menghitung tf, kemudian menghitung df dan idf. Langkah terakhir menghitung nilai tf.idf. Mari kita belajar! Tabel tf: Menghitung document frequency (df) Document frequency (df) adalah banyaknya dokumen dimana suatu term (t) muncul. Contoh: Dari soal yang sama pada menghitung tf, tentukan nilai df! Jawaban: Nilai df: Atau: Menghitung invers document frequency (idf) Contoh: Dari soal yang sama pada menghitung df, hitung nilai idf! Jawaban: Contoh: Dari soal yang sama pada menghitung df, hitung nilai idf! (dengan jumlah dokumen=N) Jawaban: Contoh: Dari soal yang sama pada menghitung df, hitung nilai idf! (N=1000) Jawaban: Menghitung tf.idf Hasil kali tf x idf Contoh: Dari soal yang sama pada menghitung df, hitung nilai tf.idf! (dengan jumlah dokumen=N) Jawaban: Untuk codenya, ketikkan seperti dibawah ini : # calculating TF-IDF vectorizer = TfidfVectorizer() tfidf_matrix = vectorizer.fit_transform(corpus) feature_name = vectorizer.get_feature_names() #print(tfidf_matrix) write_csv(\"tfidf.csv\", [feature_name]) write_csv(\"tfidf.csv\", tfidf_matrix.toarray(), 'a')","title":"Ekstraksi Teks"},{"location":"Ekstraksi_Teks/#ekstraksi-teks","text":"Tahapan selanjutnya adalah Ekstraksi Teks. Sama seperti sebelumya yaitu saya menggunakan bahasa pemrograman Python versi 3.6. Tahapan - tahapan dalam ekstraksi teks yaitu : Tahap Stop word Removal Stop word adalah kata umum (common words) yang biasanya muncul dalam jumlah besar dan dianggap tidak memiliki makna. Stop word umumnya dimanfaatkan dalam task information retrieval. Contoh stop word untuk bahasa Inggris diantaranya \u201cof\u201d, \u201cthe\u201d. Sedangkan untuk bahasa Indonesia diantaranya \u201cyang\u201d, \u201cdi\u201d, \u201cke\u201d. Tahap Stemming Stemming merupakan pemotongan suatu kata menjadi kata dasar, jadi jika terdapat kata imbuhan, maka imbuhan akan dihilangkan, dan hanya diambil kata dasarnya saja. Contohnya kata \"tersaring\" menjadi \"saring\". Agar dapat melakukan kedua tahapan diatas, maka perlu menginstall library Sastrawi dan Sklearn. Caranya adalah, pertama - tama membuka command prompt di laptop anda : Ketikkan kode berikut : pip install Sastrawi pip install Sastrawi digunakan untuk menginstall Sastrawi. pip install Sklearn pip install Sklearn digunakan untuk menginstall Sklearn Setelah Sastrawi dan Sklearn telah terinstall, kalian bisa membuka Python, lalu Ketikkan code dibawah ini : from Sastrawi.Stemmer.StemmerFactory import StemmerFactory from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory # For processing purpose from sklearn.feature_extraction.text import CountVectorizer from sklearn.feature_extraction.text import TfidfVectorizer Code diatas digunakan untuk mengambil semua library yang telah terinstall Setelah mengimport seluruh library yang dibutuhkan, maka langkah selanjutnya adalah ketikkan code dibawah ini : #VSM def countWord(txt): ''' Fungsi ini digunakan untuk menghitung setiap kata pada satu string ''' d = dict() for i in txt.split(): if d.get(i) == None: d[i] = txt.count(i) return d def add_row_VSM(d): ''' Fungsi ini digunakan untuk membangun VSM ''' #init baris baru VSM.append([]) # memasukkan kata berdasarkan kata yang telah ditemukan sebelumnya for i in VSM[0]: if d.get(i) == None: VSM[-1].append(0) else : VSM[-1].append(d.pop(i)); # memasukkan kata baru for i in d: VSM[0].append(i) for j in range(1, len(VSM)-1): VSM[j].insert(-2,0) VSM[-1].append(d.get(i)) Fungsi dari countword untuk menghitung banyaknya kata pada setiap dokumen, sedangkan fungsi add_row_vsm adalah membuat sebuah matrix vsm. Contoh dokumen untuk penghitungan kata pada setiap dokumen adalah sebagai berikut : No dahulu kala ada seekor kelinci dan tikus 1 1 1 3 3 2 1 1 2 0 0 2 1 2 1 0 3 0 1 1 1 1 1 1 4 1 1 0 2 1 2 2 Selanjutnya adalah mengetikkan code dibawah ini : cursor = conn.execute(\"SELECT * from Berita\") cursor = cursor.fetchall() #cursor = cursor[:10] pertama = True corpus = list() c=1 for row in cursor: print ('Proses : %.2f' %((c/len(cursor))*100) + '%'); c+=1 txt = row[0] cleaned = preprosesing(txt) corpus.append(cleaned) ''' d = countWord(cleaned) if pertama: pertama = False VSM = list((list(), list())) for key in d: VSM[0].append(key) VSM[1].append(d[key]) else: add_row_VSM(d) ''' Code diatas berguna untuk pemanggilan fungsi - fungsi pada code yang terdapat fungsi countword nya. Lalu, menampilkan hasilnya dengan di ekspor ke csv agar nantinya lebih mudah untuk dibaca. Caranya adalah dengan mengetikkan code dibawah ini : def write_csv(nama_file, isi, tipe='w'): 'tipe=w; write; tipe=a; append;' with open(nama_file, mode=tipe) as tbl: tbl_writer = csv.writer(tbl, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) for row in isi: tbl_writer.writerow(row) Berikut merupakan penjelasan TFIDF :","title":"Ekstraksi Teks"},{"location":"Ekstraksi_Teks/#perhitungan-tfidf","text":"Menghitung Term Frequency (tf). Term frequency (tf) merupakan frekuensi kemunculan term (t) pada dokumen (d). Contoh: Terdapat kalimat: Saya sedang belajar menghitung tf.idf. Tf.idf merupakan frekuensi kemunculan term pada dokumen. Langkah awal perhitungan tersebut adalah menghitung tf, kemudian menghitung df dan idf. Langkah terakhir menghitung nilai tf.idf. Mari kita belajar! Catatan: tiap kalimat dianggap sebagai dokumen. Tentukan nilai tf! Jawaban: Jadi dokumen tiap kalimat ditandai: Saya sedang belajar menghitung tf.idf. Tf.idf merupakan frekuensi kemunculan term pada dokumen. Langkah awal perhitungan tersebut adalah menghitung tf, kemudian menghitung df dan idf. Langkah terakhir menghitung nilai tf.idf. Mari kita belajar! Tabel tf: Menghitung document frequency (df) Document frequency (df) adalah banyaknya dokumen dimana suatu term (t) muncul. Contoh: Dari soal yang sama pada menghitung tf, tentukan nilai df! Jawaban: Nilai df: Atau: Menghitung invers document frequency (idf) Contoh: Dari soal yang sama pada menghitung df, hitung nilai idf! Jawaban: Contoh: Dari soal yang sama pada menghitung df, hitung nilai idf! (dengan jumlah dokumen=N) Jawaban: Contoh: Dari soal yang sama pada menghitung df, hitung nilai idf! (N=1000) Jawaban: Menghitung tf.idf Hasil kali tf x idf Contoh: Dari soal yang sama pada menghitung df, hitung nilai tf.idf! (dengan jumlah dokumen=N) Jawaban: Untuk codenya, ketikkan seperti dibawah ini : # calculating TF-IDF vectorizer = TfidfVectorizer() tfidf_matrix = vectorizer.fit_transform(corpus) feature_name = vectorizer.get_feature_names() #print(tfidf_matrix) write_csv(\"tfidf.csv\", [feature_name]) write_csv(\"tfidf.csv\", tfidf_matrix.toarray(), 'a')","title":"Perhitungan TFIDF"},{"location":"Kesimpulan/","text":"Kesimpulan \u00b6 Hasil dari program pada tahapan - tahapan sebelumnya menggunakan metode k-means clustering. Metode k-means clustering berusaha mengelompokkan data yang ada ke dalam beberapa kelompok, dimana data dalam satu kelompok mempunyai karakteristik yang sama satu sama lainnya dan mempunyai karakteristik yang berbeda dengan data yang ada di dalam kelompok yang lain. Jadi untuk mendapatkan sebuah cluster dengan karakteristik yang lebih mirip, caranya adalah dengan menambahkan jumlah cluster saat proses clustering. Pada saat proses clustering, peran seleksi fitur sangatlah penting karena dengan adanya seleksi fitur, fitur yang banyak dapat dilakukan pengurangan. Nah, pengurangan fitur ini pada program saya menggunakan metode pearson correlation. Metode pearson correlation tergolong metode korelasi yang sederhana dikarenakan memiliki nilai dalam range -1 \u2264 r \u2264 +1, maksudnya adalah dengan nilai paling kecil - 1 dan nilai paling besar +1. Penjelasannya sebagai berikut : Dengan besaran nilai paling kecil -1 dan paling besar +1, maka jika nilai 0 artinya tidak ada korelasi sama sekali sementara jika korelasi 1 berarti ada korelasi sempurna. Hal ini menunjukkan bahwa semakin nilai pearson correlation mendekati 1 atau -1 maka hubungan antara dua variable semakin kuat. Sebaliknya, jika nilai r atau pearson correlation mendekati 0 berarti hubungan dua variable menjadi semakin lemah. Metode pearson correlation merupakan metode yang sederhana maka dari itu untuk hasil dari program ini belum akurat, maka saya menyarankan agar sebisa mungkin mencoba melakukan percobaan dengan metode lain untuk mencari hasil yang lebih akurat, dengan begitu dapat melakukan perbandingan untuk mencapai hasil yang lebih akurat menggunakan metode pearson correlation dengan metode lain.","title":"Kesimpulan"},{"location":"Kesimpulan/#kesimpulan","text":"Hasil dari program pada tahapan - tahapan sebelumnya menggunakan metode k-means clustering. Metode k-means clustering berusaha mengelompokkan data yang ada ke dalam beberapa kelompok, dimana data dalam satu kelompok mempunyai karakteristik yang sama satu sama lainnya dan mempunyai karakteristik yang berbeda dengan data yang ada di dalam kelompok yang lain. Jadi untuk mendapatkan sebuah cluster dengan karakteristik yang lebih mirip, caranya adalah dengan menambahkan jumlah cluster saat proses clustering. Pada saat proses clustering, peran seleksi fitur sangatlah penting karena dengan adanya seleksi fitur, fitur yang banyak dapat dilakukan pengurangan. Nah, pengurangan fitur ini pada program saya menggunakan metode pearson correlation. Metode pearson correlation tergolong metode korelasi yang sederhana dikarenakan memiliki nilai dalam range -1 \u2264 r \u2264 +1, maksudnya adalah dengan nilai paling kecil - 1 dan nilai paling besar +1. Penjelasannya sebagai berikut : Dengan besaran nilai paling kecil -1 dan paling besar +1, maka jika nilai 0 artinya tidak ada korelasi sama sekali sementara jika korelasi 1 berarti ada korelasi sempurna. Hal ini menunjukkan bahwa semakin nilai pearson correlation mendekati 1 atau -1 maka hubungan antara dua variable semakin kuat. Sebaliknya, jika nilai r atau pearson correlation mendekati 0 berarti hubungan dua variable menjadi semakin lemah. Metode pearson correlation merupakan metode yang sederhana maka dari itu untuk hasil dari program ini belum akurat, maka saya menyarankan agar sebisa mungkin mencoba melakukan percobaan dengan metode lain untuk mencari hasil yang lebih akurat, dengan begitu dapat melakukan perbandingan untuk mencapai hasil yang lebih akurat menggunakan metode pearson correlation dengan metode lain.","title":"Kesimpulan"},{"location":"Pentingnya_Preprocessing/","text":"\u00b6 Pentingnya Preprocessing \u00b6 Mengapa perlu data preprocessing? Data mentah yang ada sebagian besar kotor Tidak komplet Berisi data yang hilang / kosong Kekurangan atribut yang sesuai Hanya berisi data aggregate Banyak \u201cnoise\u201d Berisi data yang Outlier Berisi error Tidak konsisten Berisi nilai yang berbeda dalam suatu kode atau nama Mengapa Data Preprocessing Penting ? Data yang tidak berkualitas, akan menghasilkan kualitas mining yang tidak baik pula. Data Preprocessing, cleaning, dan transformasi merupakan pekerjaan mayoritas dalam aplikasi data mining (90%).","title":"Pentingnya Preprocessing"},{"location":"Pentingnya_Preprocessing/#pentingnya-preprocessing","text":"Mengapa perlu data preprocessing? Data mentah yang ada sebagian besar kotor Tidak komplet Berisi data yang hilang / kosong Kekurangan atribut yang sesuai Hanya berisi data aggregate Banyak \u201cnoise\u201d Berisi data yang Outlier Berisi error Tidak konsisten Berisi nilai yang berbeda dalam suatu kode atau nama Mengapa Data Preprocessing Penting ? Data yang tidak berkualitas, akan menghasilkan kualitas mining yang tidak baik pula. Data Preprocessing, cleaning, dan transformasi merupakan pekerjaan mayoritas dalam aplikasi data mining (90%).","title":"Pentingnya Preprocessing"},{"location":"Preprocessing/","text":"Preprocessing \u00b6 Tahapan selanjutnya adalah Preprocessing / Cleaning. Proses cleaning / preprocessing mencakup antara lain membuang duplikasi data, memeriksa data yang inkonsisten, dan memperbaiki kesalahan pada data. Pada halaman ini saya akan membahas salah satu bentuk dari preprocessing yaitu seleksi fitur. Apa fungsi dari Seleksi Fitur ? Seleksi Fitur \u00b6 Tugas utama seleksi fitur adalah menentukan istilah-istilah yang layak dijadikan term index atau dengan kata lain membuang (menghilangkan) istilah-istilah yang tidak mungkin dijadikan indeks. Terdapat beberapa cara yang dapat dilakukan untuk mengeliminasi istilah-istilah yang kurang merepresentasikan dokumen tersebut, diantaranya adalah menghilangkan istilah-istilah yang sering muncul pada berbagai dokumen. Istilah-istilah yang sering muncul pada berbagai dokumen biasanya adalah istilah-istilah yang tidak mempunyai arti terhadap dokumen tersebut, jika istilah ini dihilangkan, tidak mengurangi makna dokumennya. Kata sambung seperti dan, atau dan juga merupakan contoh dari kaat sambung. Disamping istilah yang sering muncul, istilah-istilah yang jarang muncul, atau hanya muncul satu atau dua kali pada satu dokumen juga merupakan kandidat istilah yang dapat dihilangkan. Pada halaman ini, seleksi fitur saya menggunakan metode Pearson Correlation. Apa yang dimaksud metode Pearson Correlation pada seleksi fitur ? Pearson Correlation \u00b6 Korelasi Pearson merupakan salah satu ukuran korelasi yang digunakan untuk mengukur kekuatan dan arah hubungan linier dari dua veriabel. Dua variabel dikatakan berkorelasi apabila perubahan salah satu variabel disertai dengan perubahan variabel lainnya, baik dalam arah yang sama ataupun arah yang sebaliknya. Harus diingat bahwa nilai koefisien korelasi yang kecil (tidak signifikan) bukan berarti kedua variabel tersebut tidak saling berhubungan . Mungkin saja dua variabel mempunyai keeratan hubungan yang kuat namun nilai koefisien korelasinya mendekati nol, misalnya pada kasus hubungan non linier . Dengan demikian, koefisien korelasi hanya mengukur kekuatan hubungan linier dan tidak pada hubungan non linier**. **Harus diingat pula bahwa adanya hubungan linier yang kuat di antara variabel tidak selalu berarti ada hubungan kausalitas, sebab-akibat. Rumus dari Pearson Correlation adalah sebagai berikut : Rumus yang dipergunakan untuk menghitung pearson correlation adalah sebagai berikut : (Rumus ini disebut juga dengan Pearson Product Moment) r = n\u03a3xy \u2013 (\u03a3x) (\u03a3y) . \u221a{n\u03a3x\u00b2 \u2013 (\u03a3x)\u00b2} {n\u03a3y2 \u2013 (\u03a3y)2} Dimana : n = Banyaknya Pasangan data X dan Y \u03a3x = Total Jumlah dari Variabel X \u03a3y = Total Jumlah dari Variabel Y \u03a3x2= Kuadrat dari Total Jumlah Variabel X \u03a3y2= Kuadrat dari Total Jumlah Variabel Y \u03a3xy= Hasil Perkalian dari Total Jumlah Variabel X dan Variabel Y Code untuk Pearson Correlation : def pearsonCalculate(data, u,v): \"i, j is an index\" atas=0; bawah_kiri=0; bawah_kanan = 0 for k in range(len(data)): atas += (data[k,u] - meanFitur[u]) * (data[k,v] - meanFitur[v]) bawah_kiri += (data[k,u] - meanFitur[u])**2 bawah_kanan += (data[k,v] - meanFitur[v])**2 bawah_kiri = bawah_kiri ** 0.5 bawah_kanan = bawah_kanan ** 0.5 return atas/(bawah_kiri * bawah_kanan) def meanF(data): meanFitur=[] for i in range(len(data[0])): meanFitur.append(sum(data[:,i])/len(data)) return np.array(meanFitur) def seleksiFiturPearson(katadasar, data, threshold): global meanFitur meanFitur = meanF(data) u=0 while u < len(data[0]): dataBaru=data[:, :u+1] meanBaru=meanFitur[:u+1] kataBaru=katadasar[:u+1] v = u while v < len(data[0]): if u != v: value = pearsonCalculate(data, u,v) if value < threshold: dataBaru = np.hstack((dataBaru, data[:, v].reshape(data.shape[0],1))) meanBaru = np.hstack((meanBaru, meanFitur[v])) kataBaru = np.hstack((kataBaru, katadasar[v])) v+=1 data = dataBaru meanFitur=meanBaru katadasar=kataBaru if u%50 == 0 : print(\"seleksi : \", u, data.shape) u+=1 return data,kataBaru Pemanggilan function pada code diatas hanya perlu memanggil function SeleksiFiturPearson() dengan parameter data yang akan diseleksi (type data harus numpy array) dan batas nilai korelasi yang akan digunakan untuk membuang fitur yang mirip. Misalnya seperti ini: tf = numpy.array(tfidf) batas = 0.8 fiturBaru = seleksiFiturPearson(tfidf, batas) Output yang dihasilkan berupa fitur - fitur yang telah diseleksi","title":"Preprocessing"},{"location":"Preprocessing/#preprocessing","text":"Tahapan selanjutnya adalah Preprocessing / Cleaning. Proses cleaning / preprocessing mencakup antara lain membuang duplikasi data, memeriksa data yang inkonsisten, dan memperbaiki kesalahan pada data. Pada halaman ini saya akan membahas salah satu bentuk dari preprocessing yaitu seleksi fitur. Apa fungsi dari Seleksi Fitur ?","title":"Preprocessing"},{"location":"Preprocessing/#seleksi-fitur","text":"Tugas utama seleksi fitur adalah menentukan istilah-istilah yang layak dijadikan term index atau dengan kata lain membuang (menghilangkan) istilah-istilah yang tidak mungkin dijadikan indeks. Terdapat beberapa cara yang dapat dilakukan untuk mengeliminasi istilah-istilah yang kurang merepresentasikan dokumen tersebut, diantaranya adalah menghilangkan istilah-istilah yang sering muncul pada berbagai dokumen. Istilah-istilah yang sering muncul pada berbagai dokumen biasanya adalah istilah-istilah yang tidak mempunyai arti terhadap dokumen tersebut, jika istilah ini dihilangkan, tidak mengurangi makna dokumennya. Kata sambung seperti dan, atau dan juga merupakan contoh dari kaat sambung. Disamping istilah yang sering muncul, istilah-istilah yang jarang muncul, atau hanya muncul satu atau dua kali pada satu dokumen juga merupakan kandidat istilah yang dapat dihilangkan. Pada halaman ini, seleksi fitur saya menggunakan metode Pearson Correlation. Apa yang dimaksud metode Pearson Correlation pada seleksi fitur ?","title":"Seleksi Fitur"},{"location":"Preprocessing/#pearson-correlation","text":"Korelasi Pearson merupakan salah satu ukuran korelasi yang digunakan untuk mengukur kekuatan dan arah hubungan linier dari dua veriabel. Dua variabel dikatakan berkorelasi apabila perubahan salah satu variabel disertai dengan perubahan variabel lainnya, baik dalam arah yang sama ataupun arah yang sebaliknya. Harus diingat bahwa nilai koefisien korelasi yang kecil (tidak signifikan) bukan berarti kedua variabel tersebut tidak saling berhubungan . Mungkin saja dua variabel mempunyai keeratan hubungan yang kuat namun nilai koefisien korelasinya mendekati nol, misalnya pada kasus hubungan non linier . Dengan demikian, koefisien korelasi hanya mengukur kekuatan hubungan linier dan tidak pada hubungan non linier**. **Harus diingat pula bahwa adanya hubungan linier yang kuat di antara variabel tidak selalu berarti ada hubungan kausalitas, sebab-akibat. Rumus dari Pearson Correlation adalah sebagai berikut : Rumus yang dipergunakan untuk menghitung pearson correlation adalah sebagai berikut : (Rumus ini disebut juga dengan Pearson Product Moment) r = n\u03a3xy \u2013 (\u03a3x) (\u03a3y) . \u221a{n\u03a3x\u00b2 \u2013 (\u03a3x)\u00b2} {n\u03a3y2 \u2013 (\u03a3y)2} Dimana : n = Banyaknya Pasangan data X dan Y \u03a3x = Total Jumlah dari Variabel X \u03a3y = Total Jumlah dari Variabel Y \u03a3x2= Kuadrat dari Total Jumlah Variabel X \u03a3y2= Kuadrat dari Total Jumlah Variabel Y \u03a3xy= Hasil Perkalian dari Total Jumlah Variabel X dan Variabel Y Code untuk Pearson Correlation : def pearsonCalculate(data, u,v): \"i, j is an index\" atas=0; bawah_kiri=0; bawah_kanan = 0 for k in range(len(data)): atas += (data[k,u] - meanFitur[u]) * (data[k,v] - meanFitur[v]) bawah_kiri += (data[k,u] - meanFitur[u])**2 bawah_kanan += (data[k,v] - meanFitur[v])**2 bawah_kiri = bawah_kiri ** 0.5 bawah_kanan = bawah_kanan ** 0.5 return atas/(bawah_kiri * bawah_kanan) def meanF(data): meanFitur=[] for i in range(len(data[0])): meanFitur.append(sum(data[:,i])/len(data)) return np.array(meanFitur) def seleksiFiturPearson(katadasar, data, threshold): global meanFitur meanFitur = meanF(data) u=0 while u < len(data[0]): dataBaru=data[:, :u+1] meanBaru=meanFitur[:u+1] kataBaru=katadasar[:u+1] v = u while v < len(data[0]): if u != v: value = pearsonCalculate(data, u,v) if value < threshold: dataBaru = np.hstack((dataBaru, data[:, v].reshape(data.shape[0],1))) meanBaru = np.hstack((meanBaru, meanFitur[v])) kataBaru = np.hstack((kataBaru, katadasar[v])) v+=1 data = dataBaru meanFitur=meanBaru katadasar=kataBaru if u%50 == 0 : print(\"seleksi : \", u, data.shape) u+=1 return data,kataBaru Pemanggilan function pada code diatas hanya perlu memanggil function SeleksiFiturPearson() dengan parameter data yang akan diseleksi (type data harus numpy array) dan batas nilai korelasi yang akan digunakan untuk membuang fitur yang mirip. Misalnya seperti ini: tf = numpy.array(tfidf) batas = 0.8 fiturBaru = seleksiFiturPearson(tfidf, batas) Output yang dihasilkan berupa fitur - fitur yang telah diseleksi","title":"Pearson Correlation"},{"location":"Referensi/","text":"Referensi \u00b6 Berikut ini terlampir referensi dari tahapan - tahapan yang telah dibuat : https://www.dictio.id/t/apa-yang-dimaksud-dengan-web-crawler/15116 https://scrapy.org/ https://www.octoparse.com/blog/how-to-build-a-web-crawler-from-scratch-a-guide-for-beginners https://www.laserfiche.com/support/webhelp/quickfields/8.0/en-us/content/text%20extraction.htm https://www.researchgate.net/publication/292286778_A_Effective_Preprocessing_for_Web_Usage_Mining https://pdfs.semanticscholar.org/39ea/984a452ed12992a8a494cc9e1df65545b2fa.pdf http://ilmukomputer.org/wp-content/uploads/2018/05/agus-k-means-clustering.pdf https://piptools.net/algoritma-fcm-fuzzy-c-means-clustering/","title":"Referensi"},{"location":"Referensi/#referensi","text":"Berikut ini terlampir referensi dari tahapan - tahapan yang telah dibuat : https://www.dictio.id/t/apa-yang-dimaksud-dengan-web-crawler/15116 https://scrapy.org/ https://www.octoparse.com/blog/how-to-build-a-web-crawler-from-scratch-a-guide-for-beginners https://www.laserfiche.com/support/webhelp/quickfields/8.0/en-us/content/text%20extraction.htm https://www.researchgate.net/publication/292286778_A_Effective_Preprocessing_for_Web_Usage_Mining https://pdfs.semanticscholar.org/39ea/984a452ed12992a8a494cc9e1df65545b2fa.pdf http://ilmukomputer.org/wp-content/uploads/2018/05/agus-k-means-clustering.pdf https://piptools.net/algoritma-fcm-fuzzy-c-means-clustering/","title":"Referensi"},{"location":"Selvie Akmalia/","text":"Crawling \u00b6 Berikut ini, saya akan membahas tutorial mengambil data (crawl) dari sebuah Website dan disimpan ke database. Sebelumnya, disini saya menggunakan bahasa pemrograman Python. Bagi kalian yang belum mempunyai Python, kalian bisa mendownload di internet lalu menginstallnya di laptop masing - masing. Dibawah ini adalah tahapan - tahapan untuk mengcrawl data : Pertama - tama download Python di internet (kalian bisa memilih ingin mendownload versi berapa), saya memilih mendownload versi 3.6 Setelah Python terdownload, waktunya menginstall di laptop kalian masing - masing Sebelum menjalankan Python, anda perlu menginstall library request dan beautifulsoup dengan cara : Membuka command prompt di laptop anda Ketikkan kode berikut : pip install bs4 pip install bs4 digunakan untuk menginstall beautifulsoup. pip install requests pip install request digunakan untuk menginstall requests Mengapa kita harus menginstall beautifulsoup dan requests ? Kita perlu menginstall beautifulsoup untuk mengubah objek file ke beautifulsoup, sedangkan requests untuk mengambil data dari internet Setelah requests dan beautifulsoup telah terinstall, kalian bisa membuka Python yang telah terinstall Ketikkan code dibawah ini : import requests from bs4 import BeautifulSoup import sqlite3 Code diatas digunakan untuk mengambil semua library yang telah terinstall Setelah mengimport seluruh library yang dibutuhkan, maka langkah selanjutnya adalah mengkoneksikan database ke file .db yang telah dibuat sebelumnya, codenya seperti dibawah ini : conn = sqlite3.connect('test.db') Melakukan pengecekan jika terdapat nama tabel yang sama, tabel yang telah saya buat namanya adalah \"Berita\", jadi jika terdapat tabel yang namanya juga \"Berita\"maka otomatis akan dihapus, codenya seperti dibawah ini : conn.execute('drop table if exists Berita') Membuat tabel dengan nama sesuai keinginan, tabel disini saya beri nama \"Berita\" dengan field \"Judul\" dan \"Isi\". Codenya seperti dibawah ini : conn.execute('''CREATE TABLE Berita (Judul TEXT NOT NULL, Isi TEXT NOT NULL);''') Membuat variabel yang isinya adalah link URL dari website yang ingin kita ambil datanya. Codenya seperti dibawah ini : src = \"https://www.anehdidunia.com/\" Link URL yang saya ambil adalah www.anehdidunia.com Setelah itu kita melakukan konversi pada link URL diatas dan mengubahnya ke objek beautifulsoup. Codenya seperti dibawah ini : page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') FIeld yang telah kita buat tadi yaitu \"Judul\" dan \"Isi\" akan mengambil data berdasarkan class yang terdapat pada Website yang kita ambil. Untuk mengetahui class tersebut, kita cukup memblok teks yang akan kita crawl lalu klik kanan, setelah itu klik \"Inspect\" maka akan muncul class yang ingin kita ambil. Codenya seperti dibawah ini : judul = soup.findAll(class_='post-title entry-title') isi = soup.findAll(class_='snippets') Dengan menggunakan perulangan for, kita akan membuat variabel baru yang bernama a dan b kemudian mendeklarasikan ke tipe data \"string\" dengan deklarasi \"%s\" dan \"%s\". Codenya seperti dibawah ini : for i in range(len(judul)): a = judul[i].getText() b = isi[i].getText() conn.execute('INSERT INTO Berita(Judul, Isi) VALUES (\"%s\", \"%s\")' %(a, b)); Membuat variabel yang isinya akan menampilkan seluruh isi yang terdapat pada tabel yang bernama \"Berita\". Codenya seperti dibawah ini : cursor = conn.execute(\"SELECT * from Berita\") Menampilkan baris yang telah diambil pada tabel \"Berita\". Codenya seperti dibawah ini : for row in cursor: print(row)","title":"Crawling"},{"location":"Selvie Akmalia/#crawling","text":"Berikut ini, saya akan membahas tutorial mengambil data (crawl) dari sebuah Website dan disimpan ke database. Sebelumnya, disini saya menggunakan bahasa pemrograman Python. Bagi kalian yang belum mempunyai Python, kalian bisa mendownload di internet lalu menginstallnya di laptop masing - masing. Dibawah ini adalah tahapan - tahapan untuk mengcrawl data : Pertama - tama download Python di internet (kalian bisa memilih ingin mendownload versi berapa), saya memilih mendownload versi 3.6 Setelah Python terdownload, waktunya menginstall di laptop kalian masing - masing Sebelum menjalankan Python, anda perlu menginstall library request dan beautifulsoup dengan cara : Membuka command prompt di laptop anda Ketikkan kode berikut : pip install bs4 pip install bs4 digunakan untuk menginstall beautifulsoup. pip install requests pip install request digunakan untuk menginstall requests Mengapa kita harus menginstall beautifulsoup dan requests ? Kita perlu menginstall beautifulsoup untuk mengubah objek file ke beautifulsoup, sedangkan requests untuk mengambil data dari internet Setelah requests dan beautifulsoup telah terinstall, kalian bisa membuka Python yang telah terinstall Ketikkan code dibawah ini : import requests from bs4 import BeautifulSoup import sqlite3 Code diatas digunakan untuk mengambil semua library yang telah terinstall Setelah mengimport seluruh library yang dibutuhkan, maka langkah selanjutnya adalah mengkoneksikan database ke file .db yang telah dibuat sebelumnya, codenya seperti dibawah ini : conn = sqlite3.connect('test.db') Melakukan pengecekan jika terdapat nama tabel yang sama, tabel yang telah saya buat namanya adalah \"Berita\", jadi jika terdapat tabel yang namanya juga \"Berita\"maka otomatis akan dihapus, codenya seperti dibawah ini : conn.execute('drop table if exists Berita') Membuat tabel dengan nama sesuai keinginan, tabel disini saya beri nama \"Berita\" dengan field \"Judul\" dan \"Isi\". Codenya seperti dibawah ini : conn.execute('''CREATE TABLE Berita (Judul TEXT NOT NULL, Isi TEXT NOT NULL);''') Membuat variabel yang isinya adalah link URL dari website yang ingin kita ambil datanya. Codenya seperti dibawah ini : src = \"https://www.anehdidunia.com/\" Link URL yang saya ambil adalah www.anehdidunia.com Setelah itu kita melakukan konversi pada link URL diatas dan mengubahnya ke objek beautifulsoup. Codenya seperti dibawah ini : page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') FIeld yang telah kita buat tadi yaitu \"Judul\" dan \"Isi\" akan mengambil data berdasarkan class yang terdapat pada Website yang kita ambil. Untuk mengetahui class tersebut, kita cukup memblok teks yang akan kita crawl lalu klik kanan, setelah itu klik \"Inspect\" maka akan muncul class yang ingin kita ambil. Codenya seperti dibawah ini : judul = soup.findAll(class_='post-title entry-title') isi = soup.findAll(class_='snippets') Dengan menggunakan perulangan for, kita akan membuat variabel baru yang bernama a dan b kemudian mendeklarasikan ke tipe data \"string\" dengan deklarasi \"%s\" dan \"%s\". Codenya seperti dibawah ini : for i in range(len(judul)): a = judul[i].getText() b = isi[i].getText() conn.execute('INSERT INTO Berita(Judul, Isi) VALUES (\"%s\", \"%s\")' %(a, b)); Membuat variabel yang isinya akan menampilkan seluruh isi yang terdapat pada tabel yang bernama \"Berita\". Codenya seperti dibawah ini : cursor = conn.execute(\"SELECT * from Berita\") Menampilkan baris yang telah diambil pada tabel \"Berita\". Codenya seperti dibawah ini : for row in cursor: print(row)","title":"Crawling"},{"location":"carakerja_web_crawler/","text":"Cara Kerja Web Crawler \u00b6 Web crawler atau yang dikenal juga dengan istilah web spider bertugas untuk mengumpulkan semua informasi yang ada di dalam halaman web. Web crawler bekerja secara otomatis dengan cara memberikan sejumlah alamat website untuk dikunjungi serta menyimpan semua informasi yang terkandung didalamnya. Setiap kali web crawler mengunjungi sebuah website, maka web crawler tersebut akan mendata semua link yang ada dihalaman yang dikunjunginya itu untuk kemudian di kunjungi lagi satu persatu. Proses web crawler dalam mengunjungi setiap dokumen web disebut dengan web crawling atau spidering. Beberapa websites, khususnya yang berhubungan dengan pencarian menggunakan proses spidering untuk memperbaharui data data mereka. Web crawler biasa digunakan untuk membuat salinan secara sebagian atau keseluruhan halaman web yang telah dikunjunginya agar dapat di proses lebih lanjut oleh system pengindexan. Crawler dapat juga digunakan untuk proses pemeliharaan sebuah website, seperti memvalidasi kode html sebuah web, dan crawler juga digunakan untuk memperoleh data yang khusus seperti mengumpulkan alamat e-mail. Web crawler termasuk kedalam bagian software agent atau yang lebih dikenal dengan istilah program bot. Secara umum crawler memulai prosesnya dengan memberikan daftar sejumlah alamat website untuk dikunjungi, disebut sebagai seeds. Setiap kali sebuah halaman web dikunjungi, crawler akan mencari alamat yang lain yang terdapat didalamnya dan menambahkan kedalam daftar seeds sebelumnya. Secara umum crawler memulai prosesnya dengan memberikan daftar sejumlah alamat website untuk dikunjungi, disebut sebagai seeds. Setiap kali sebuah halaman web dikunjungi, crawler akan mencari alamat yang lain yang terdapat didalamnya dan menambahkan kedalam daftar seeds sebelumnya. Dalam melakukan prosesnya, web crawler juga mempunyai beberapa persoalan yang harus mampu di atasinya. Permasalahan tersebut mencakup : Halaman mana yang harus dikunjungi terlebih dahulu. Aturan dalam proses mengunjungi kembali sebuah halaman. Performansi, mencakup banyaknya halaman yang harus dikunjungi. Aturan dalam setiap kunjungan agar server yang dikunjungi tidak kelebihan beban. Kegagalan, mencakup tidak tersedianya halaman yang dikunjungi, server down, timeout, maupun jebakan yang sengaja dibuat oleh webmaster. Seberapa jauh kedalaman sebuah website yang akan dikunjungi. Hal yang tak kalah pentingnya adalah kemampuan web crawler untuk mengikuti Perkembangan teknologi web, dimana setiap kali teknologi baru muncul, web crawler harus dapat menyesuaikan diri agar dapat mengunjungi halaman web yang menggunakan teknologi baru tersebut. Proses sebuah web crawler untuk mendata link \u2013 link yang terdapat didalam sebuah halaman web menggunakan pendekatan regular expression. Crawler akan menelurusi setiap karakter yang ada untuk menemukan hyperlink tag html. Setiap hyperlink tag yang ditemukan diperiksa lebih lanjut apakah tag tersebut mengandung atribut nofollow rel, jika tidak ada maka diambil nilai yang terdapat didalam attribute href yang merupakan sebuah link baru. Setelah proses crawler selesai di lanjutkan dengan indexing system yang bertugas untuk menganalisa halaman web yang telah tersimpan sebelumnya dengan cara mengindeks setiap kemungkinan term yang terdapat di dalamnnya. Data term yang ditemukan disimpan dalam sebuah database indeks untuk digunakan dalam pencarian selanjutnya. Indexing system mengumpulkan, memilah dan menyimpan data untuk memberikan kemudahan dalam pengaksesan informasi secara tepat dan akurat. Proses pengolahan halaman web agar dapat digunakan untuk proses pencarian berikutnya dinakamakan web indexing. Dalam implementasinya index system dirancang dari penggabungan beberapa cabang ilmu antara lain ilmu bahasa, psikologi, matematika, informatika, fisika, dan ilmu komputer. Tujuan dari penyimpanan data berupa indeks adalah untuk performansi dan kecepatan dalam menemukan informasi yang relevan berdasarkan inputan user. Tanpa adanya indeks, search engine harus melakukan scan terhadap setiap dokumen yang ada didalam database. Hal ini tentu saja akan membutuhkan proses sumber daya yang sangat besar dalam proses komputasi. Sebagai contoh, indeks dari 10.000 dokumen dapat diproses dalam waktu beberapa detik saja, sedangkan penulusuran secara berurutan setiap kata yang terdapat di dalam 10.000 dokumen akan membutuhkan waktu yang berjam lamanya. Tempat tambahan mungkin akan dibutuhkan di dalam computer untuk penyimpanan indeks, tapi hal ini akan terbayar dengan penghematan waktu pada saat pemrosesan pencarian dokumen yang dibutuhkan.","title":"Cara Kerja Web Crawler"},{"location":"carakerja_web_crawler/#cara-kerja-web-crawler","text":"Web crawler atau yang dikenal juga dengan istilah web spider bertugas untuk mengumpulkan semua informasi yang ada di dalam halaman web. Web crawler bekerja secara otomatis dengan cara memberikan sejumlah alamat website untuk dikunjungi serta menyimpan semua informasi yang terkandung didalamnya. Setiap kali web crawler mengunjungi sebuah website, maka web crawler tersebut akan mendata semua link yang ada dihalaman yang dikunjunginya itu untuk kemudian di kunjungi lagi satu persatu. Proses web crawler dalam mengunjungi setiap dokumen web disebut dengan web crawling atau spidering. Beberapa websites, khususnya yang berhubungan dengan pencarian menggunakan proses spidering untuk memperbaharui data data mereka. Web crawler biasa digunakan untuk membuat salinan secara sebagian atau keseluruhan halaman web yang telah dikunjunginya agar dapat di proses lebih lanjut oleh system pengindexan. Crawler dapat juga digunakan untuk proses pemeliharaan sebuah website, seperti memvalidasi kode html sebuah web, dan crawler juga digunakan untuk memperoleh data yang khusus seperti mengumpulkan alamat e-mail. Web crawler termasuk kedalam bagian software agent atau yang lebih dikenal dengan istilah program bot. Secara umum crawler memulai prosesnya dengan memberikan daftar sejumlah alamat website untuk dikunjungi, disebut sebagai seeds. Setiap kali sebuah halaman web dikunjungi, crawler akan mencari alamat yang lain yang terdapat didalamnya dan menambahkan kedalam daftar seeds sebelumnya. Secara umum crawler memulai prosesnya dengan memberikan daftar sejumlah alamat website untuk dikunjungi, disebut sebagai seeds. Setiap kali sebuah halaman web dikunjungi, crawler akan mencari alamat yang lain yang terdapat didalamnya dan menambahkan kedalam daftar seeds sebelumnya. Dalam melakukan prosesnya, web crawler juga mempunyai beberapa persoalan yang harus mampu di atasinya. Permasalahan tersebut mencakup : Halaman mana yang harus dikunjungi terlebih dahulu. Aturan dalam proses mengunjungi kembali sebuah halaman. Performansi, mencakup banyaknya halaman yang harus dikunjungi. Aturan dalam setiap kunjungan agar server yang dikunjungi tidak kelebihan beban. Kegagalan, mencakup tidak tersedianya halaman yang dikunjungi, server down, timeout, maupun jebakan yang sengaja dibuat oleh webmaster. Seberapa jauh kedalaman sebuah website yang akan dikunjungi. Hal yang tak kalah pentingnya adalah kemampuan web crawler untuk mengikuti Perkembangan teknologi web, dimana setiap kali teknologi baru muncul, web crawler harus dapat menyesuaikan diri agar dapat mengunjungi halaman web yang menggunakan teknologi baru tersebut. Proses sebuah web crawler untuk mendata link \u2013 link yang terdapat didalam sebuah halaman web menggunakan pendekatan regular expression. Crawler akan menelurusi setiap karakter yang ada untuk menemukan hyperlink tag html. Setiap hyperlink tag yang ditemukan diperiksa lebih lanjut apakah tag tersebut mengandung atribut nofollow rel, jika tidak ada maka diambil nilai yang terdapat didalam attribute href yang merupakan sebuah link baru. Setelah proses crawler selesai di lanjutkan dengan indexing system yang bertugas untuk menganalisa halaman web yang telah tersimpan sebelumnya dengan cara mengindeks setiap kemungkinan term yang terdapat di dalamnnya. Data term yang ditemukan disimpan dalam sebuah database indeks untuk digunakan dalam pencarian selanjutnya. Indexing system mengumpulkan, memilah dan menyimpan data untuk memberikan kemudahan dalam pengaksesan informasi secara tepat dan akurat. Proses pengolahan halaman web agar dapat digunakan untuk proses pencarian berikutnya dinakamakan web indexing. Dalam implementasinya index system dirancang dari penggabungan beberapa cabang ilmu antara lain ilmu bahasa, psikologi, matematika, informatika, fisika, dan ilmu komputer. Tujuan dari penyimpanan data berupa indeks adalah untuk performansi dan kecepatan dalam menemukan informasi yang relevan berdasarkan inputan user. Tanpa adanya indeks, search engine harus melakukan scan terhadap setiap dokumen yang ada didalam database. Hal ini tentu saja akan membutuhkan proses sumber daya yang sangat besar dalam proses komputasi. Sebagai contoh, indeks dari 10.000 dokumen dapat diproses dalam waktu beberapa detik saja, sedangkan penulusuran secara berurutan setiap kata yang terdapat di dalam 10.000 dokumen akan membutuhkan waktu yang berjam lamanya. Tempat tambahan mungkin akan dibutuhkan di dalam computer untuk penyimpanan indeks, tapi hal ini akan terbayar dengan penghematan waktu pada saat pemrosesan pencarian dokumen yang dibutuhkan.","title":"Cara Kerja Web Crawler"},{"location":"teknik_preprocessing/","text":"Teknik Preprocessing \u00b6 Apa Saja Teknik Data Preprocessing ? Teknik Data Preprocessing : Data Cleaning Data integration Data Reduction Data Transformation Data Cleaning Proses untuk membersihkan data dengan beberapa teknik, yaitu : Memperkecil noise Membetulkan data yang tidak konsisten Mengisi missing value Mengidentifikasi atau membuang outlier Data Cleaning : Missing Values Mengabaikan record Biasanya untuk label klasifikasi yang kosong Mengisikan secara manual Menggunakan mean/median dari atribut yang mengandung missing value Mean dapat dipakai jika distribusi data normal Median digunakan jika distribusi data tidak normal ( condong ) Menggunakan nilai global Menggunakan nilai termungkin Menerapkan regresi Data Cleaning : Noisy Data Noise data adalah suatu kesalahan acak atau variasi dalam variabel terukur Teknik-teknik Binning Smoothing by bin means Smoothing by bin medians Smoothing by bin boundaries Regression Outlier Analysis Data Integration Data dapat bersumber dari beberapa sumber Teknik Analisis korelasi Atribut redudan Duplikasi Data Transformation Tujuannya diharapkan lebih efisien dalam proses data mining dan mungkin juga agar pola yang dihasilkan lebih mudah dipahami Strategi Smoothing Attribute (feature) construction Aggregation Normalization Discretization","title":"Teknik Preprocessing"},{"location":"teknik_preprocessing/#teknik-preprocessing","text":"Apa Saja Teknik Data Preprocessing ? Teknik Data Preprocessing : Data Cleaning Data integration Data Reduction Data Transformation Data Cleaning Proses untuk membersihkan data dengan beberapa teknik, yaitu : Memperkecil noise Membetulkan data yang tidak konsisten Mengisi missing value Mengidentifikasi atau membuang outlier Data Cleaning : Missing Values Mengabaikan record Biasanya untuk label klasifikasi yang kosong Mengisikan secara manual Menggunakan mean/median dari atribut yang mengandung missing value Mean dapat dipakai jika distribusi data normal Median digunakan jika distribusi data tidak normal ( condong ) Menggunakan nilai global Menggunakan nilai termungkin Menerapkan regresi Data Cleaning : Noisy Data Noise data adalah suatu kesalahan acak atau variasi dalam variabel terukur Teknik-teknik Binning Smoothing by bin means Smoothing by bin medians Smoothing by bin boundaries Regression Outlier Analysis Data Integration Data dapat bersumber dari beberapa sumber Teknik Analisis korelasi Atribut redudan Duplikasi Data Transformation Tujuannya diharapkan lebih efisien dalam proses data mining dan mungkin juga agar pola yang dihasilkan lebih mudah dipahami Strategi Smoothing Attribute (feature) construction Aggregation Normalization Discretization","title":"Teknik Preprocessing"}]}