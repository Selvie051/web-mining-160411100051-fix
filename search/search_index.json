{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Selvie Akmalia 160411100051 Penambangan dan Pencarian Web - Dosen Pengampu Bapak Mula'ab, S.si, M.kom Teknik Informatika Universitas Trunojoyo Madura Pengantar \u00b6 Apa itu Web Crawler ? Web Crawler adalah sebuah program dengan metode tertentu yang berfungsi untuk melakukan Scan atau crawl ke semua halaman internet untuk mencari data yang diinginkan. Data tersebut merupakan hasil teratas dari mesin pencari google dan yahoo. Web Crawler atau web spider, web robot, bot crawl dan automathic indeker. Web clawer memiliki beberapa maafaat dan tujuan tetapi penggunaan yang paling umum digunakan sebagai search engine. Search engine merupakan sebuah tool yang berfungsi sebagai mesin pencari seperti google. Kelebihan Web Crawler Kelebihan Suatu program atau script yang relatif simple Proses sebuah web crawler untuk mendata link \u2013 link yang terdapat didalam sebuah halaman web menggunakan pendekatan regular expression. Crawler akan menelurusi setiap karakter yang ada untuk menemukan hyperlink tag html. Setiap hyperlink tag yang ditemukan diperiksa lebih lanjut apakah tag tersebut mengandung atribut nofollow rel, jika tidak ada maka diambil nilai yang terdapat didalam attribute href yang merupakan sebuah link baru. Dapat digunakan untuk beragam tujuan. Penggunaan yang paling umum adalah yang terkait atau berhubungan langsung dengan search engine. Kekurangan Mengingat dia sejatinya mesin atau program, kita mesti menyesuaikan (semisal konten yang berguna) agar web crawler juga tahu bahwa halaman yang sedang dijelajahinya berisi informasi bermanfaat. Sebab, mesin web crawlerc menandai proses-proses tertentu dalam situs sebagai kelemahan. Misalnya, halaman-halaman yang lamban saat dipanggil mungkin dilewati begitu saja. Atau, lantaran konfigurasi situs tidak terlalu bagus membuat si web crawler kurang efisien kala mengeksplorasi. Lalu, dia lebih fokus pada beberapa halaman dan melewatkan yang lain padahal mungkin isinya penting. Ini akan menjadi tantangan tersendiri bagi situs ritel besar yang banyak mengandung konten dinamis, yaitu halaman web yang kompleks (misalnya konten yang dipersonalisasi). Tools & Environment \u00b6 Menggunakan bahasa pemrograman Python versi 3.6 Library yang digunakan untuk crawling : requests BeautifulSoup4 (bs4) Library untuk menyimpan data : sqlite (database) csv (import data ke csv) Library yang digunakan untuk preprocessing : Sastrawi Library yang digunakan untuk seleksi fitur & clustering : scipy numpy sklearn (Scikit-learn) skfuzzy (Scikit-fuzzy) Website target : https://www.anehdidunia.com/ Running program membutuhkan koneksi internet untuk mengambil data dari website target","title":"Home"},{"location":"#pengantar","text":"Apa itu Web Crawler ? Web Crawler adalah sebuah program dengan metode tertentu yang berfungsi untuk melakukan Scan atau crawl ke semua halaman internet untuk mencari data yang diinginkan. Data tersebut merupakan hasil teratas dari mesin pencari google dan yahoo. Web Crawler atau web spider, web robot, bot crawl dan automathic indeker. Web clawer memiliki beberapa maafaat dan tujuan tetapi penggunaan yang paling umum digunakan sebagai search engine. Search engine merupakan sebuah tool yang berfungsi sebagai mesin pencari seperti google. Kelebihan Web Crawler Kelebihan Suatu program atau script yang relatif simple Proses sebuah web crawler untuk mendata link \u2013 link yang terdapat didalam sebuah halaman web menggunakan pendekatan regular expression. Crawler akan menelurusi setiap karakter yang ada untuk menemukan hyperlink tag html. Setiap hyperlink tag yang ditemukan diperiksa lebih lanjut apakah tag tersebut mengandung atribut nofollow rel, jika tidak ada maka diambil nilai yang terdapat didalam attribute href yang merupakan sebuah link baru. Dapat digunakan untuk beragam tujuan. Penggunaan yang paling umum adalah yang terkait atau berhubungan langsung dengan search engine. Kekurangan Mengingat dia sejatinya mesin atau program, kita mesti menyesuaikan (semisal konten yang berguna) agar web crawler juga tahu bahwa halaman yang sedang dijelajahinya berisi informasi bermanfaat. Sebab, mesin web crawlerc menandai proses-proses tertentu dalam situs sebagai kelemahan. Misalnya, halaman-halaman yang lamban saat dipanggil mungkin dilewati begitu saja. Atau, lantaran konfigurasi situs tidak terlalu bagus membuat si web crawler kurang efisien kala mengeksplorasi. Lalu, dia lebih fokus pada beberapa halaman dan melewatkan yang lain padahal mungkin isinya penting. Ini akan menjadi tantangan tersendiri bagi situs ritel besar yang banyak mengandung konten dinamis, yaitu halaman web yang kompleks (misalnya konten yang dipersonalisasi).","title":"Pengantar"},{"location":"#tools-environment","text":"Menggunakan bahasa pemrograman Python versi 3.6 Library yang digunakan untuk crawling : requests BeautifulSoup4 (bs4) Library untuk menyimpan data : sqlite (database) csv (import data ke csv) Library yang digunakan untuk preprocessing : Sastrawi Library yang digunakan untuk seleksi fitur & clustering : scipy numpy sklearn (Scikit-learn) skfuzzy (Scikit-fuzzy) Website target : https://www.anehdidunia.com/ Running program membutuhkan koneksi internet untuk mengambil data dari website target","title":"Tools &amp; Environment"},{"location":"Clustering/","text":"Clustering \u00b6 Tahapan selanjutnya adalah Clustering. Sama seperti sebelumya yaitu saya menggunakan bahasa pemrograman Python versi 3.6. Apa itu clustering ? Clustering adalah metode penganalisaan data, yang sering dimasukkan sebagai salah satu metode Data Mining , yang tujuannya adalah untuk mengelompokkan data dengan karakteristik yang sama ke suatu \u2018wilayah\u2019 yang sama dan data dengan karakteristik yang berbeda ke \u2018wilayah\u2019 yang lain. Clustering Dengan Pendekatan Partisi : K-Means \u00b6 Salah satu metode yang banyak digunakan dalam melakukan clustering dengan partisi ini adalah metode k-means. Secara umum metode k-means ini melakukan proses pengelompokan dengan prosedur sebagai berikut: Tentukan jumlah cluster Alokasikan data secara random ke cluster yang ada Hitung rata-rata setiap cluster dari data yang tergabung di dalamnya Alokasikan kembali semua data ke cluster terdekat Ulang proses nomor 3, sampai tidak ada perubahan atau perubahan yang terjadi masih sudah di bawah treshold Prosedur dasar ini bisa berubah mengikuti pendekatan pengalokasian data yang diterapkan, apakah crisp atau fuzzy . Setelah meneliti clustering dari sudut yang lain, saya menemukan bahwa k-means clustering mempunyai beberapa kelemahan. Mixture Modelling (Mixture Modeling) Mixture modelling (mixture modeling) merupakan metode pengelompokan data yang mirip dengan k-means dengan kelebihan penggunaan distribusi statistik dalam mendefinisikan setiap cluster yang ditemukan. Dibandingkan dengan k-means yang hanya menggunakan cluster center, penggunaan distribusi statistik ini mengijinkan kita untuk: Memodel data yang kita miliki dengan setting karakteristik yang berbeda-beda Jumlah cluster yang sesuai dengan keadaan data bisa ditemukan seiring dengan proses pemodelan karakteristik dari masing-masing cluster Hasil pemodelan clustering yang dilaksanakan bisa diuji tingkat keakuratannya Distribusi statistik yang digunakan bisa bermacam-macam mulai dari yang digunakan untuk data categorical sampai yang continuous, termasuk di antaranya distribusi binomial, multinomial, normal dan lain-lain. Beberapa distribusi yang bersifat tidak normal seperti distribusi Poisson, von-Mises, Gamma dan Student t, juga diterapkan untuk bisa mengakomodasi berbagai keadaan data yang ada di lapangan. Beberapa pendekatan multivariate juga banyak diterapkan untuk memperhitungkan tingkat keterkaitan antara variabel data yang satu dengan yang lainnya. Data clustering menggunakan metode K-Means Clustering ini secara umum dilakukan dengan algoritma dasar sebagai berikut: Tentukan jumlah cluster Alokasikan data ke dalam cluster secara random Hitung centroid/rata-rata dari data yang ada di masing-masing cluster Alokasikan masing-masing data ke centroid/rata-rata terdekat Kembali ke Step 3, apabila masih ada data yang berpindah cluster atau apabila perubahan nilai centroid, ada yang di atas nilai threshold yang ditentukan atau apabila perubahan nilai pada objective function yang digunakan di atas nilai threshold yang ditentukan Code dibawah ini digunakan untuk melakukan clustering, sebagai contoh, cluster dibagi menjadi 5. Namun, nantinya cluster dapat diubah sesuai dengan kebutuhan. # Clustering kmeans = KMeans(n_clusters=5, random_state=0).fit(fiturBaru) write_csv(\"Kluster_label.csv\", [kmeans.labels_]) s_avg = silhouette_score(fiturBaru, kmeans.labels_, random_state=10) print(s_avg) for i in range(len(kmeans.labels_)): print(\"Doc %d =>> cluster %d\" %(i+1, kmeans.labels_[i])) Fuzzy K-Means \u00b6 Metode Fuzzy K-Means (atau lebih sering disebut sebagai Fuzzy C-Means ) mengalokasikan kembali data ke dalam masing-masing cluster dengan memanfaatkan teori Fuzzy. Teori ini mengeneralisasikan metode pengalokasian yang bersifat tegas (hard) seperti yang digunakan pada metode Hard K-Means. Dalam metode Fuzzy K-Means dipergunakan variabel membership function, uik, yang merujuk pada seberapa besar kemungkinan suatu data bisa menjadi anggota ke dalam suatu cluster. Konsep dari Fuzzy C-Means pertama kali adalah menentukan pusat cluster, yang akan menandai lokasi rata-rata untuk tiap-tiap cluster. Pada kondisi awal, pusat cluster ini masih belum akurat. Tiap-tiap titik data memiliki derajat keanggotaan untuk tiap-tiap cluster. Dengan cara memperbaiki pusat cluster dan derajat keanggotaan tiap-tiap titik data secara berulang, maka akan dapat dilihat bahwa pusat cluster akan bergerak menuju lokasi yang tepat. Perulangan ini didasarkan pada minimasi fungsi obyektif yang menggambarkan jarak dari titik data yang diberikan kepusat cluster yang terbobot oleh derajat keanggotaan titik data tersebut. Output dari Fuzzy C-Means merupakan deretan usat cluster dan beberapa derajat keanggotaan untuk tiap-tiap titik data. Informasi ini dapat digunakan untuk membangun suatu fuzzy inference system. Algoritma Fuzzy K-Means adalah sebagai berikut : Shilhoutte Coefficient** \u00b6 Metode Shilhoutte Coefficient ini berfungsi untuk menguji kualitas dari cluster yang dihasilkan. Metode ini merupakan metode validasi cluster yang menggabungkan metode cohesion dan Separation . Untuk menghitung nilai silhoutte coefisient diperlukan jarak antar dokumen dengan menggunakan rumus EuclideanDistance . Setelah itu tahapan untuk menghitung nilai silhoutte coeffisien adalah sebagai berikut : Untuk setiap objek i, hitung rata-rata jarak dari objek i dengan seluruh objek yang berada dalam satu cluster. Akan didapatkan nilai rata-rata yang disebut a*i*. Untuk setiap objek i, hitung rata-rata jarak dari objek i dengan objek yang berada di cluster lainnya. Dari semua jarak rata-rata tersebut ambil nilai yang paling kecil. Nilai ini disebut b*i*. Setelah itu maka untuk objek i memiliki nilai silhoutte coefisien : S*i*= (b*i* \u2013 a*i*) / max(a*i*, b*i*) Hasil perhitungan nilai silhoutte coeffisien dapat bervariasi antara -1 hingga 1. Hasil clustering dikatakan baik jikai nilai silhoutte coeffisien bernilai positif (a*i* < b*i*) dan a*i* mendekati 0, sehingga akan menghasilkan nilai silhoutte coeffisien yang maksimum yaitu 1 saat a*i* = 0. Maka dapat dikatakan, jika s*i* = 1 berarti objek i sudah berada dalam cluster yang tepat. Jika nilai s*i* = 0 maka objek i*berada di antara dua cluster sehingga objek tersebut tidak jelas harus dimasukan ke dalam cluster A atau cluster B. Akan tetapi, jika s*i = -1 artinya struktur cluster yang dihasilkan overlapping, sehingga objek i lebih tepat dimasukan ke dalam cluster yang lain. Nilai rata-rata silhoutte coeffisien dari tiap objek dalam suatu cluster adalah suatu ukuran yang menunjukan seberapa ketat data dikelompokan dalam cluster tersebut. Berikut adalah nilai silhoutte berdasarkan Kaufman dan Rousseeuw : 0.7 < SC <= 1 Strong Stucture 0.5 < SC <= 0.7 Medium Structure 0.25 < SC <= 0.5 Weak Structure SC <= 0.25 No structure Code untuk shilhoutte coefficient : s_avg = silhouette_score(fiturBaru, kmeans.labels_, random_state=10)","title":"Clustering"},{"location":"Clustering/#clustering","text":"Tahapan selanjutnya adalah Clustering. Sama seperti sebelumya yaitu saya menggunakan bahasa pemrograman Python versi 3.6. Apa itu clustering ? Clustering adalah metode penganalisaan data, yang sering dimasukkan sebagai salah satu metode Data Mining , yang tujuannya adalah untuk mengelompokkan data dengan karakteristik yang sama ke suatu \u2018wilayah\u2019 yang sama dan data dengan karakteristik yang berbeda ke \u2018wilayah\u2019 yang lain. Clustering Dengan Pendekatan Partisi :","title":"Clustering"},{"location":"Clustering/#k-means","text":"Salah satu metode yang banyak digunakan dalam melakukan clustering dengan partisi ini adalah metode k-means. Secara umum metode k-means ini melakukan proses pengelompokan dengan prosedur sebagai berikut: Tentukan jumlah cluster Alokasikan data secara random ke cluster yang ada Hitung rata-rata setiap cluster dari data yang tergabung di dalamnya Alokasikan kembali semua data ke cluster terdekat Ulang proses nomor 3, sampai tidak ada perubahan atau perubahan yang terjadi masih sudah di bawah treshold Prosedur dasar ini bisa berubah mengikuti pendekatan pengalokasian data yang diterapkan, apakah crisp atau fuzzy . Setelah meneliti clustering dari sudut yang lain, saya menemukan bahwa k-means clustering mempunyai beberapa kelemahan. Mixture Modelling (Mixture Modeling) Mixture modelling (mixture modeling) merupakan metode pengelompokan data yang mirip dengan k-means dengan kelebihan penggunaan distribusi statistik dalam mendefinisikan setiap cluster yang ditemukan. Dibandingkan dengan k-means yang hanya menggunakan cluster center, penggunaan distribusi statistik ini mengijinkan kita untuk: Memodel data yang kita miliki dengan setting karakteristik yang berbeda-beda Jumlah cluster yang sesuai dengan keadaan data bisa ditemukan seiring dengan proses pemodelan karakteristik dari masing-masing cluster Hasil pemodelan clustering yang dilaksanakan bisa diuji tingkat keakuratannya Distribusi statistik yang digunakan bisa bermacam-macam mulai dari yang digunakan untuk data categorical sampai yang continuous, termasuk di antaranya distribusi binomial, multinomial, normal dan lain-lain. Beberapa distribusi yang bersifat tidak normal seperti distribusi Poisson, von-Mises, Gamma dan Student t, juga diterapkan untuk bisa mengakomodasi berbagai keadaan data yang ada di lapangan. Beberapa pendekatan multivariate juga banyak diterapkan untuk memperhitungkan tingkat keterkaitan antara variabel data yang satu dengan yang lainnya. Data clustering menggunakan metode K-Means Clustering ini secara umum dilakukan dengan algoritma dasar sebagai berikut: Tentukan jumlah cluster Alokasikan data ke dalam cluster secara random Hitung centroid/rata-rata dari data yang ada di masing-masing cluster Alokasikan masing-masing data ke centroid/rata-rata terdekat Kembali ke Step 3, apabila masih ada data yang berpindah cluster atau apabila perubahan nilai centroid, ada yang di atas nilai threshold yang ditentukan atau apabila perubahan nilai pada objective function yang digunakan di atas nilai threshold yang ditentukan Code dibawah ini digunakan untuk melakukan clustering, sebagai contoh, cluster dibagi menjadi 5. Namun, nantinya cluster dapat diubah sesuai dengan kebutuhan. # Clustering kmeans = KMeans(n_clusters=5, random_state=0).fit(fiturBaru) write_csv(\"Kluster_label.csv\", [kmeans.labels_]) s_avg = silhouette_score(fiturBaru, kmeans.labels_, random_state=10) print(s_avg) for i in range(len(kmeans.labels_)): print(\"Doc %d =>> cluster %d\" %(i+1, kmeans.labels_[i]))","title":"K-Means"},{"location":"Clustering/#fuzzy-k-means","text":"Metode Fuzzy K-Means (atau lebih sering disebut sebagai Fuzzy C-Means ) mengalokasikan kembali data ke dalam masing-masing cluster dengan memanfaatkan teori Fuzzy. Teori ini mengeneralisasikan metode pengalokasian yang bersifat tegas (hard) seperti yang digunakan pada metode Hard K-Means. Dalam metode Fuzzy K-Means dipergunakan variabel membership function, uik, yang merujuk pada seberapa besar kemungkinan suatu data bisa menjadi anggota ke dalam suatu cluster. Konsep dari Fuzzy C-Means pertama kali adalah menentukan pusat cluster, yang akan menandai lokasi rata-rata untuk tiap-tiap cluster. Pada kondisi awal, pusat cluster ini masih belum akurat. Tiap-tiap titik data memiliki derajat keanggotaan untuk tiap-tiap cluster. Dengan cara memperbaiki pusat cluster dan derajat keanggotaan tiap-tiap titik data secara berulang, maka akan dapat dilihat bahwa pusat cluster akan bergerak menuju lokasi yang tepat. Perulangan ini didasarkan pada minimasi fungsi obyektif yang menggambarkan jarak dari titik data yang diberikan kepusat cluster yang terbobot oleh derajat keanggotaan titik data tersebut. Output dari Fuzzy C-Means merupakan deretan usat cluster dan beberapa derajat keanggotaan untuk tiap-tiap titik data. Informasi ini dapat digunakan untuk membangun suatu fuzzy inference system. Algoritma Fuzzy K-Means adalah sebagai berikut :","title":"Fuzzy K-Means"},{"location":"Clustering/#shilhoutte-coefficient","text":"Metode Shilhoutte Coefficient ini berfungsi untuk menguji kualitas dari cluster yang dihasilkan. Metode ini merupakan metode validasi cluster yang menggabungkan metode cohesion dan Separation . Untuk menghitung nilai silhoutte coefisient diperlukan jarak antar dokumen dengan menggunakan rumus EuclideanDistance . Setelah itu tahapan untuk menghitung nilai silhoutte coeffisien adalah sebagai berikut : Untuk setiap objek i, hitung rata-rata jarak dari objek i dengan seluruh objek yang berada dalam satu cluster. Akan didapatkan nilai rata-rata yang disebut a*i*. Untuk setiap objek i, hitung rata-rata jarak dari objek i dengan objek yang berada di cluster lainnya. Dari semua jarak rata-rata tersebut ambil nilai yang paling kecil. Nilai ini disebut b*i*. Setelah itu maka untuk objek i memiliki nilai silhoutte coefisien : S*i*= (b*i* \u2013 a*i*) / max(a*i*, b*i*) Hasil perhitungan nilai silhoutte coeffisien dapat bervariasi antara -1 hingga 1. Hasil clustering dikatakan baik jikai nilai silhoutte coeffisien bernilai positif (a*i* < b*i*) dan a*i* mendekati 0, sehingga akan menghasilkan nilai silhoutte coeffisien yang maksimum yaitu 1 saat a*i* = 0. Maka dapat dikatakan, jika s*i* = 1 berarti objek i sudah berada dalam cluster yang tepat. Jika nilai s*i* = 0 maka objek i*berada di antara dua cluster sehingga objek tersebut tidak jelas harus dimasukan ke dalam cluster A atau cluster B. Akan tetapi, jika s*i = -1 artinya struktur cluster yang dihasilkan overlapping, sehingga objek i lebih tepat dimasukan ke dalam cluster yang lain. Nilai rata-rata silhoutte coeffisien dari tiap objek dalam suatu cluster adalah suatu ukuran yang menunjukan seberapa ketat data dikelompokan dalam cluster tersebut. Berikut adalah nilai silhoutte berdasarkan Kaufman dan Rousseeuw : 0.7 < SC <= 1 Strong Stucture 0.5 < SC <= 0.7 Medium Structure 0.25 < SC <= 0.5 Weak Structure SC <= 0.25 No structure Code untuk shilhoutte coefficient : s_avg = silhouette_score(fiturBaru, kmeans.labels_, random_state=10)","title":"Shilhoutte Coefficient**"},{"location":"Ekstraksi_Teks/","text":"Ekstraksi Teks \u00b6 Tahapan selanjutnya adalah Ekstraksi Teks. Sama seperti sebelumya yaitu saya menggunakan bahasa pemrograman Python versi 3.6. Tahapan - tahapan dalam ekstraksi teks yaitu : Tahap Stop word Removal Stop word adalah kata umum (common words) yang biasanya muncul dalam jumlah besar dan dianggap tidak memiliki makna. Stop word umumnya dimanfaatkan dalam task information retrieval. Contoh stop word untuk bahasa Inggris diantaranya \u201cof\u201d, \u201cthe\u201d. Sedangkan untuk bahasa Indonesia diantaranya \u201cyang\u201d, \u201cdi\u201d, \u201cke\u201d. Tahap Stemming Stemming merupakan pemotongan suatu kata menjadi kata dasar, jadi jika terdapat kata imbuhan, maka imbuhan akan dihilangkan, dan hanya diambil kata dasarnya saja. Contohnya kata \"tersaring\" menjadi \"saring\". Agar dapat melakukan kedua tahapan diatas, maka perlu menginstall library Sastrawi dan Sklearn. Caranya adalah, pertama - tama membuka command prompt di laptop anda : Ketikkan kode berikut : pip install Sastrawi pip install Sastrawi digunakan untuk menginstall Sastrawi. pip install Sklearn pip install Sklearn digunakan untuk menginstall Sklearn Setelah Sastrawi dan Sklearn telah terinstall, kalian bisa membuka Python, lalu Ketikkan code dibawah ini : from Sastrawi.Stemmer.StemmerFactory import StemmerFactory from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory # For processing purpose from sklearn.feature_extraction.text import CountVectorizer from sklearn.feature_extraction.text import TfidfVectorizer Code diatas digunakan untuk mengambil semua library yang telah terinstall Setelah mengimport seluruh library yang dibutuhkan, maka langkah selanjutnya adalah ketikkan code dibawah ini : #VSM def countWord(txt): ''' Fungsi ini digunakan untuk menghitung setiap kata pada satu string ''' d = dict() for i in txt.split(): if d.get(i) == None: d[i] = txt.count(i) return d def add_row_VSM(d): ''' Fungsi ini digunakan untuk membangun VSM ''' #init baris baru VSM.append([]) # memasukkan kata berdasarkan kata yang telah ditemukan sebelumnya for i in VSM[0]: if d.get(i) == None: VSM[-1].append(0) else : VSM[-1].append(d.pop(i)); # memasukkan kata baru for i in d: VSM[0].append(i) for j in range(1, len(VSM)-1): VSM[j].insert(-2,0) VSM[-1].append(d.get(i)) Fungsi dari countword untuk menghitung banyaknya kata pada setiap dokumen, sedangkan fungsi add_row_vsm adalah membuat sebuah matrix vsm. Contoh dokumen untuk penghitungan kata pada setiap dokumen adalah sebagai berikut : No dahulu kala ada seekor kelinci dan tikus 1 1 1 3 3 2 1 1 2 0 0 2 1 2 1 0 3 0 1 1 1 1 1 1 4 1 1 0 2 1 2 2 Selanjutnya adalah mengetikkan code dibawah ini : cursor = conn.execute(\"SELECT * from Berita\") cursor = cursor.fetchall() #cursor = cursor[:10] pertama = True corpus = list() c=1 for row in cursor: print ('Proses : %.2f' %((c/len(cursor))*100) + '%'); c+=1 txt = row[0] cleaned = preprosesing(txt) corpus.append(cleaned) ''' d = countWord(cleaned) if pertama: pertama = False VSM = list((list(), list())) for key in d: VSM[0].append(key) VSM[1].append(d[key]) else: add_row_VSM(d) ''' Code diatas berguna untuk pemanggilan fungsi - fungsi pada code yang terdapat fungsi countword nya. Lalu, menampilkan hasilnya dengan di ekspor ke csv agar nantinya lebih mudah untuk dibaca. Caranya adalah dengan mengetikkan code dibawah ini : def write_csv(nama_file, isi, tipe='w'): 'tipe=w; write; tipe=a; append;' with open(nama_file, mode=tipe) as tbl: tbl_writer = csv.writer(tbl, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) for row in isi: tbl_writer.writerow(row) Berikut merupakan penjelasan TFIDF : Perhitungan TFIDF \u00b6 Menghitung Term Frequency (tf). Term frequency (tf) merupakan frekuensi kemunculan term (t) pada dokumen (d). Contoh: Terdapat kalimat: Saya sedang belajar menghitung tf.idf. Tf.idf merupakan frekuensi kemunculan term pada dokumen. Langkah awal perhitungan tersebut adalah menghitung tf, kemudian menghitung df dan idf. Langkah terakhir menghitung nilai tf.idf. Mari kita belajar! Catatan: tiap kalimat dianggap sebagai dokumen. Tentukan nilai tf! Jawaban: Jadi dokumen tiap kalimat ditandai: Saya sedang belajar menghitung tf.idf. Tf.idf merupakan frekuensi kemunculan term pada dokumen. Langkah awal perhitungan tersebut adalah menghitung tf, kemudian menghitung df dan idf. Langkah terakhir menghitung nilai tf.idf. Mari kita belajar! Tabel tf: Menghitung document frequency (df) Document frequency (df) adalah banyaknya dokumen dimana suatu term (t) muncul. Contoh: Dari soal yang sama pada menghitung tf, tentukan nilai df! Jawaban: Nilai df: Atau: Menghitung invers document frequency (idf) Contoh: Dari soal yang sama pada menghitung df, hitung nilai idf! Jawaban: Contoh: Dari soal yang sama pada menghitung df, hitung nilai idf! (dengan jumlah dokumen=N) Jawaban: Contoh: Dari soal yang sama pada menghitung df, hitung nilai idf! (N=1000) Jawaban: Menghitung tf.idf Hasil kali tf x idf Contoh: Dari soal yang sama pada menghitung df, hitung nilai tf.idf! (dengan jumlah dokumen=N) Jawaban: Untuk codenya, ketikkan seperti dibawah ini : # calculating TF-IDF vectorizer = TfidfVectorizer() tfidf_matrix = vectorizer.fit_transform(corpus) feature_name = vectorizer.get_feature_names() #print(tfidf_matrix) write_csv(\"tfidf.csv\", [feature_name]) write_csv(\"tfidf.csv\", tfidf_matrix.toarray(), 'a')","title":"Ekstraksi Teks"},{"location":"Ekstraksi_Teks/#ekstraksi-teks","text":"Tahapan selanjutnya adalah Ekstraksi Teks. Sama seperti sebelumya yaitu saya menggunakan bahasa pemrograman Python versi 3.6. Tahapan - tahapan dalam ekstraksi teks yaitu : Tahap Stop word Removal Stop word adalah kata umum (common words) yang biasanya muncul dalam jumlah besar dan dianggap tidak memiliki makna. Stop word umumnya dimanfaatkan dalam task information retrieval. Contoh stop word untuk bahasa Inggris diantaranya \u201cof\u201d, \u201cthe\u201d. Sedangkan untuk bahasa Indonesia diantaranya \u201cyang\u201d, \u201cdi\u201d, \u201cke\u201d. Tahap Stemming Stemming merupakan pemotongan suatu kata menjadi kata dasar, jadi jika terdapat kata imbuhan, maka imbuhan akan dihilangkan, dan hanya diambil kata dasarnya saja. Contohnya kata \"tersaring\" menjadi \"saring\". Agar dapat melakukan kedua tahapan diatas, maka perlu menginstall library Sastrawi dan Sklearn. Caranya adalah, pertama - tama membuka command prompt di laptop anda : Ketikkan kode berikut : pip install Sastrawi pip install Sastrawi digunakan untuk menginstall Sastrawi. pip install Sklearn pip install Sklearn digunakan untuk menginstall Sklearn Setelah Sastrawi dan Sklearn telah terinstall, kalian bisa membuka Python, lalu Ketikkan code dibawah ini : from Sastrawi.Stemmer.StemmerFactory import StemmerFactory from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory # For processing purpose from sklearn.feature_extraction.text import CountVectorizer from sklearn.feature_extraction.text import TfidfVectorizer Code diatas digunakan untuk mengambil semua library yang telah terinstall Setelah mengimport seluruh library yang dibutuhkan, maka langkah selanjutnya adalah ketikkan code dibawah ini : #VSM def countWord(txt): ''' Fungsi ini digunakan untuk menghitung setiap kata pada satu string ''' d = dict() for i in txt.split(): if d.get(i) == None: d[i] = txt.count(i) return d def add_row_VSM(d): ''' Fungsi ini digunakan untuk membangun VSM ''' #init baris baru VSM.append([]) # memasukkan kata berdasarkan kata yang telah ditemukan sebelumnya for i in VSM[0]: if d.get(i) == None: VSM[-1].append(0) else : VSM[-1].append(d.pop(i)); # memasukkan kata baru for i in d: VSM[0].append(i) for j in range(1, len(VSM)-1): VSM[j].insert(-2,0) VSM[-1].append(d.get(i)) Fungsi dari countword untuk menghitung banyaknya kata pada setiap dokumen, sedangkan fungsi add_row_vsm adalah membuat sebuah matrix vsm. Contoh dokumen untuk penghitungan kata pada setiap dokumen adalah sebagai berikut : No dahulu kala ada seekor kelinci dan tikus 1 1 1 3 3 2 1 1 2 0 0 2 1 2 1 0 3 0 1 1 1 1 1 1 4 1 1 0 2 1 2 2 Selanjutnya adalah mengetikkan code dibawah ini : cursor = conn.execute(\"SELECT * from Berita\") cursor = cursor.fetchall() #cursor = cursor[:10] pertama = True corpus = list() c=1 for row in cursor: print ('Proses : %.2f' %((c/len(cursor))*100) + '%'); c+=1 txt = row[0] cleaned = preprosesing(txt) corpus.append(cleaned) ''' d = countWord(cleaned) if pertama: pertama = False VSM = list((list(), list())) for key in d: VSM[0].append(key) VSM[1].append(d[key]) else: add_row_VSM(d) ''' Code diatas berguna untuk pemanggilan fungsi - fungsi pada code yang terdapat fungsi countword nya. Lalu, menampilkan hasilnya dengan di ekspor ke csv agar nantinya lebih mudah untuk dibaca. Caranya adalah dengan mengetikkan code dibawah ini : def write_csv(nama_file, isi, tipe='w'): 'tipe=w; write; tipe=a; append;' with open(nama_file, mode=tipe) as tbl: tbl_writer = csv.writer(tbl, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) for row in isi: tbl_writer.writerow(row) Berikut merupakan penjelasan TFIDF :","title":"Ekstraksi Teks"},{"location":"Ekstraksi_Teks/#perhitungan-tfidf","text":"Menghitung Term Frequency (tf). Term frequency (tf) merupakan frekuensi kemunculan term (t) pada dokumen (d). Contoh: Terdapat kalimat: Saya sedang belajar menghitung tf.idf. Tf.idf merupakan frekuensi kemunculan term pada dokumen. Langkah awal perhitungan tersebut adalah menghitung tf, kemudian menghitung df dan idf. Langkah terakhir menghitung nilai tf.idf. Mari kita belajar! Catatan: tiap kalimat dianggap sebagai dokumen. Tentukan nilai tf! Jawaban: Jadi dokumen tiap kalimat ditandai: Saya sedang belajar menghitung tf.idf. Tf.idf merupakan frekuensi kemunculan term pada dokumen. Langkah awal perhitungan tersebut adalah menghitung tf, kemudian menghitung df dan idf. Langkah terakhir menghitung nilai tf.idf. Mari kita belajar! Tabel tf: Menghitung document frequency (df) Document frequency (df) adalah banyaknya dokumen dimana suatu term (t) muncul. Contoh: Dari soal yang sama pada menghitung tf, tentukan nilai df! Jawaban: Nilai df: Atau: Menghitung invers document frequency (idf) Contoh: Dari soal yang sama pada menghitung df, hitung nilai idf! Jawaban: Contoh: Dari soal yang sama pada menghitung df, hitung nilai idf! (dengan jumlah dokumen=N) Jawaban: Contoh: Dari soal yang sama pada menghitung df, hitung nilai idf! (N=1000) Jawaban: Menghitung tf.idf Hasil kali tf x idf Contoh: Dari soal yang sama pada menghitung df, hitung nilai tf.idf! (dengan jumlah dokumen=N) Jawaban: Untuk codenya, ketikkan seperti dibawah ini : # calculating TF-IDF vectorizer = TfidfVectorizer() tfidf_matrix = vectorizer.fit_transform(corpus) feature_name = vectorizer.get_feature_names() #print(tfidf_matrix) write_csv(\"tfidf.csv\", [feature_name]) write_csv(\"tfidf.csv\", tfidf_matrix.toarray(), 'a')","title":"Perhitungan TFIDF"},{"location":"Kesimpulan/","text":"Kesimpulan \u00b6 Hasil dari program pada tahapan - tahapan sebelumnya menggunakan metode k-means clustering. Metode k-means clustering berusaha mengelompokkan data yang ada ke dalam beberapa kelompok, dimana data dalam satu kelompok mempunyai karakteristik yang sama satu sama lainnya dan mempunyai karakteristik yang berbeda dengan data yang ada di dalam kelompok yang lain. Jadi untuk mendapatkan sebuah cluster dengan karakteristik yang lebih mirip, caranya adalah dengan menambahkan jumlah cluster saat proses clustering. Pada saat proses clustering, peran seleksi fitur sangatlah penting karena dengan adanya seleksi fitur, fitur yang banyak dapat dilakukan pengurangan. Nah, pengurangan fitur ini pada program saya menggunakan metode pearson correlation. Metode pearson correlation tergolong metode korelasi yang sederhana dikarenakan memiliki nilai dalam range -1 \u2264 r \u2264 +1, maksudnya adalah dengan nilai paling kecil - 1 dan nilai paling besar +1. Penjelasannya sebagai berikut : Dengan besaran nilai paling kecil -1 dan paling besar +1, maka jika nilai 0 artinya tidak ada korelasi sama sekali sementara jika korelasi 1 berarti ada korelasi sempurna. Hal ini menunjukkan bahwa semakin nilai pearson correlation mendekati 1 atau -1 maka hubungan antara dua variable semakin kuat. Sebaliknya, jika nilai r atau pearson correlation mendekati 0 berarti hubungan dua variable menjadi semakin lemah. Metode pearson correlation merupakan metode yang sederhana maka dari itu untuk hasil dari program ini belum akurat, maka saya menyarankan agar sebisa mungkin mencoba melakukan percobaan dengan metode lain untuk mencari hasil yang lebih akurat, dengan begitu dapat melakukan perbandingan untuk mencapai hasil yang lebih akurat menggunakan metode pearson correlation dengan metode lain.","title":"Kesimpulan"},{"location":"Kesimpulan/#kesimpulan","text":"Hasil dari program pada tahapan - tahapan sebelumnya menggunakan metode k-means clustering. Metode k-means clustering berusaha mengelompokkan data yang ada ke dalam beberapa kelompok, dimana data dalam satu kelompok mempunyai karakteristik yang sama satu sama lainnya dan mempunyai karakteristik yang berbeda dengan data yang ada di dalam kelompok yang lain. Jadi untuk mendapatkan sebuah cluster dengan karakteristik yang lebih mirip, caranya adalah dengan menambahkan jumlah cluster saat proses clustering. Pada saat proses clustering, peran seleksi fitur sangatlah penting karena dengan adanya seleksi fitur, fitur yang banyak dapat dilakukan pengurangan. Nah, pengurangan fitur ini pada program saya menggunakan metode pearson correlation. Metode pearson correlation tergolong metode korelasi yang sederhana dikarenakan memiliki nilai dalam range -1 \u2264 r \u2264 +1, maksudnya adalah dengan nilai paling kecil - 1 dan nilai paling besar +1. Penjelasannya sebagai berikut : Dengan besaran nilai paling kecil -1 dan paling besar +1, maka jika nilai 0 artinya tidak ada korelasi sama sekali sementara jika korelasi 1 berarti ada korelasi sempurna. Hal ini menunjukkan bahwa semakin nilai pearson correlation mendekati 1 atau -1 maka hubungan antara dua variable semakin kuat. Sebaliknya, jika nilai r atau pearson correlation mendekati 0 berarti hubungan dua variable menjadi semakin lemah. Metode pearson correlation merupakan metode yang sederhana maka dari itu untuk hasil dari program ini belum akurat, maka saya menyarankan agar sebisa mungkin mencoba melakukan percobaan dengan metode lain untuk mencari hasil yang lebih akurat, dengan begitu dapat melakukan perbandingan untuk mencapai hasil yang lebih akurat menggunakan metode pearson correlation dengan metode lain.","title":"Kesimpulan"},{"location":"Pagerank/","text":"Pengertian Pagerank \u00b6 Algoritma dari mesin telusur google yang mempunyai fungsi untuk menganalisis link dan memberikan nomor atau peringkat ke setiap halaman web yang ada di internet inilah yang disebut pagerank . Pagerank sebuah halaman web dihitung dari 0-10, semakin besar pagerank yang dimiliki sebuah halaman web maka semakin besar peluang untuk mendapatkan posisi paling atas pada hasil penelusuran google . Namun, peluang untuk mendapatkan posisi paling atas di mesin pencari google ini tidak memiliki pengaruh yang besar seperti dulu, karena sekarang google sudah menerapkan beberapa algoritma baru yang berfungsi untuk menentukan halaman web mana yang harus menempati urutan pertama dan seterusnya. Fungsi Pagerank \u00b6 Fungsi utama dari pagerank yaitu menganalisis berbagai link yang masuk ( backlink ) yang nantinya akan dihitung berapa jumlah link yang masuk ( inbound ) dan link yang keluar ( outbound ) dari sebuah halaman web tersebut, lalu akan menghasilkan pagerank yang kita dapatkan. Fungsi lainnya dari pagerank yaitu menentukan situs web mana yang lebih populer atau lebih penting dimulai dari skala 10 (yang paling tinggi) hingga skala 0 (yang paling rendah). Semakin tinggi pagerank yang dimiliki sebuah web maka semakin populer lah halaman web tersebut, hal ini dikarenakan dengan tingginya pagerank yang dimiliki sebuah web itu berarti bahwa situs web tersebut banyak ditautkan oleh situs web lainnya. Konsep Dasar Pagerank \u00b6 Pagerank memiliki konsep dasar yang sama dengan link popularity, tetapi tidak hanya memperhitungkan \u201cjumlah\u201d inbound link dan outbound link. Pendekatan yang digunakan adalah sebuah halaman akan dianggap penting jika halaman lain memiliki link ke halaman tersebut. Sebuah halaman juga akan menjadi semakin penting jika halaman lain yang memiliki rangking ( pagerank ) tinggi mengacu ke halaman tersebut. Sistem pagerank dipandang sangat tinggi dalam hal obyektifitas dan akurasi relefansi pencariannya. Mereka menggunakan struktur linknya yang kuat sebagai indikator dari nilai sebuah halaman. Google menginterpretasikan link dari halaman A ke halaman B sebagai sebuah pilihan atau vote, oleh halaman A, untuk halaman B.Tetapi, google melihat lebih dari sekedar jumlah pilihan atau link yang diterima oleh sesuatu halaman, Google juga menganalisis halaman yang memberi pilihan tersebut. Pilihan yang diberikan oleh halaman yang penting dimana halaman tersebut berbobot lebih tinggi sehingga dapat membantu membuat halaman lain menjadi penting. Situs yang penting dan berkualitas tinggi akan memperoleh lebih banyak pagerank dan akan diingat oleh google pada tiap-tiap pencarian. Tetapi, halaman yang penting tetap saja tidak berguna bila ternyata tidak sesuai dengan keinginan seseorang. Oleh karena itu, google mengkombinasikan pagerank dengan teknik pembandingan teks untuk mencari halaman web yang penting dan relevan dengan apa yang dicari oleh seseorang. google lebih dari sekedar menghitung jumlah keberadaan kata dalam sebuah halaman dan memeriksa segala aspek dari isi halaman termasuk halaman-halaman yang terkait untuk memutuskan apakah halaman tersebut cocok dengan apa yang dicari seseorang. Yang dimaksud faktor peredam ialah faktor yang menyertakan pengguna dalam kalkulasi. Cara Menghitung Pagerank \u00b6 Dari pendekatan yang sudah dijelaskan sebelumnya mengenai konsep pagerank , algoritma pagerank dapat dirumuskan seperti di bawah ini : Algoritma awal : $$ PR(A) = (1-d) + d ( ( PR(T1)/C (T1) )+ . . .+ ( PR(Tn) / C(Tn) ) ) $$ Salah satu algoritma lain yang dipublikasikan : $$ PR(A) = (1-d) / N + d ( (PR(T1) / C(T1) ) + \u2026 + ( PR(Tn) / C (Tn) ) ) $$ Keterangan dari algoritma diatas adalah : PR(A) adalah pagerank halaman A PR(T1) adalah pagerank halaman T1 yang mengacu ke halaman A C(T1) adalah jumlah link keluar ( outbound link ) pada halaman T1 d adalah damping factor yang bisa diberi antara 0 dan 1. N adalah jumlah keseluruhan halaman web (yang terindeks oleh Google) Dari algoritma di atas dapat dilihat bahwa pagerank ditentukan untuk setiap halaman anda bukan keseluruhan situs web. Pagerank sebuah halaman ditentukan dari pagerank halaman yang mengacu kepadanya yang juga menjalani proses penentuan pagerank dengan cara yang sama, jadi proses ini akan berulang sampai ditemukan hasil yang tepat. Akan tetapi pagerank halaman A tidak langsung diberikan kepada halaman yang dituju, melainkan sebelumnya dibagi dengan jumlah link yang ada pada halaman T1 (outbound link), dan pagerank itu akan dibagi rata kepada setiap link yang ada pada halaman tersebut. Demikian juga dengan setiap halaman lain \u201cTn\u201d yang mengacu ke halaman \u201cA\u201d. Setelah semua pagerank yang didapat dari halaman-halaman lain yang mengacu ke halaman \u201cA\u201d dijumlahkan, nilai itu kemudian dikalikan dengan damping factor yang bernilai antara 0 sampai 1. Hal ini dilakukan agar tidak semua nilai pagerank halaman T didistribusikan ke halaman A. Berapa nilai PR yang benar adalah hak hitung dari google Dari algoritma di atas dapat dilihat bahwa pagerank ditentukan untuk setiap halaman bukan keseluruhan situs web. Pagerank sebuah halaman ditentukan dari pagerank halaman yang mengacu kepadanya yang juga menjalani proses penentuan pagerank dengan cara yang sama, jadi proses ini akan berulang sampai ditemukan hasil yang tepat. Akan tetapi pagerank halaman A tidak langsung diberikan kepada halaman yang dituju, akan tetapi sebelumnya dibagi dengan jumlah link yang ada pada halaman T1 (outbound link), dan pagerank itu akan dibagi rata kepada setiap link yang ada pada halaman tersebut. Demikian juga dengan setiap halaman lain \u201cTn\u201d yang mengacu ke halaman \u201cA\u201d. Setelah semua pagerank yang didapat dari halaman-halaman lain yang mengacu ke halaman \u201cA\u201d dijumlahkan, nilai itu kemudian dikalikan dengan damping factor yang bernilai antara 0 sampai 1. Hal ini dilakukan agar tidak keseluruhan nilai pagerank halaman T didistribusikan ke halaman A. Kalkulasi pagerank \u00b6 Nilai pagerank tinggi masih merupakan faktor umum untuk menilai otoritas sebuah website. Berikut ini merupakan tabel perhitungan pagerank secara manual : PR Link for PR3 Link for PR4 Link for PR5 Link for PR6 Link for PR7 Link for PR8 PR 1 555 3,005 16,803 92,414 508,277 2,795,522 PR 2 101 555 3,005 16,803 92,414 508,277 PR 3 18.5 101 555 3,005 16,803 92,414 PR 4 3.5 18.5 101 555 3,005 16,803 PR 5 1 3.5 18.5 101 555 3,005 PR 6 0.5 1 3.5 18.5 101 555 PR 7 0.5 0.5 1 3.5 18.5 101 PR 8 0.5 0.5 0.5 1 3.5 18.5 PR 9 0.5 0.5 0.5 0.5 1 3.5 PR 10 0.5 0.5 0.5 0.5 0.5 1 Dasar pagerank cukup mudah, yang perlu diketahui dalam perhitungan PR ( pagerank ) adalah: Berapa incoming link ? Berapa banyak total halaman lain terhubung ke website anda ? Bagaimana PR ( pagerank ) halaman website yang terlink ke website anda ? Contoh kalkulasi pagerank sederhana: Mari kita berasumsi bahwa seluruh web hanya terdiri atas empat website A, B, C, dan D, masing-masing memiliki nilai pagerank yaitu \u201c*1\u201d. Jumlahnya sama dengan jumlah website. Website B, C dan D masing-masing memiliki sebuah link ke website A dan tidak ada link lainnya. Apabila faktor peredam diabaikan, hasilnya adalah rumus : PR(A) = 1/1 + 1/1 dan 1/1, *pagerank website adalah 3. Contoh yang lebih rumit: Website A memiliki link ke website B dan C. B hanya memiliki sebuah link ke A. C memiliki link ke A, B dan D. D hanya memiliki link ke B. Rumus untuk A akan menjadi PR (A) = 1/1 + \u2153. Link dari B bernilai 1, sementara dari C hanya \u2153 dengan jumlah link 3, hasilnya adalah 1,33. Untuk B : PR(B) = \u00bd + \u2153 + 1/1 hasilnya 1, 83 Untuk C : PR(C) = \u00bd hasilnya 0,5 Untuk D : PR(D) = \u2153 hasilnya 0,33 Jumlah pagerank website A, B, C dan D seharusnya sama dengan jumlah website 1,33 + 1,83 + 0,5 + 0,33 = 3,99. Kekurangan 0,1 disebabkan oleh adanya pembulatan. Dalam kalkulasi ini masih ada yang kurang. Pagerank setiap website tidak disertakan. Contoh berikutnya adalah website b. Apabila kalkulasi disesuaikan dengan pagerank yang didapat dari langkah pertama . PR(B) = \u00bd +\u2153 +1/1. Didapat term berikut : PR(B) = 1,33/2 + 0,5/3 +0,33/1 hasilnya adalah 1,62. Tentu saja kalkulasi baru pagerank website B mengubah pagerank website A, C dan D. Nilai baru website D kembali mengubah nilai website B. Code Perhitungan Pagerank : \u00b6 Berikut ini merupakan code untuk menghitung pagerank : damping = 0.85 max_iterr = 100 error_toleransi = 0.01 pr = nx.pagerank(g, alpha - damping, max_iter1=max_iterr, tol1=error_toleransi) Pada penjelasan diatas telah dijabarkan bagaimana perhitungan manual pagerank. Sedangkan untuk code diatas merupakan perhitungan pagerank dengan library yang terdapat pada python yaitu networkx. Dalam networkx memiliki module pagerank yang memiliki parameter diatas yaitu g yang merupakan variabel yang nantinya menampung graph, alpha beridi nilai damping dengan default 0.85, max_iter untuk nilai maksimum iterasi yang diizinkan, terakhir yatu tol untuk menampung nilai minimal eror toleransi. Code untuk menampung hasil perhitungan ke dalam list : print(\"keterangan node:\") nodelist = g.nodes label= {} data=[] for i, key in enumerate(nodelist): data.append((pr[key], key)) label[key]=i Langkah selanjutnya yaitu mengurutkan nilai pagerank. Code untuk mengurutkan pagerank adalah sebagai berikut : urut = data.copy() for x in range(len(urut)): for y in range(len(urut)): if urut[x][0] > urut[y][0]: urut[x], urut[y] = urut[y], urut[x] urut = pd.DataFrame(data, None, (\"Pagerank\", \"Node\")) Graph \u00b6 Selanjutnya yaitu membentuk garph, graph disini berfungsi agar mendapatkan visualisasi yang lebih terstruktur dari hasil pagerank. Code membentuk graph : g = nx.from_pandas_edgelist(edgeListFrame, \"From\", \"To\", None, nx.DiGraph()) pos = nx.spring_layout(g) plt.title('Link Graph') nx.draw(g, pos) nx.draw_networkx_labels(g, pos, label, font_color=\"w\") plt.axis(\"off\") plt.show()","title":"Pagerank"},{"location":"Pagerank/#pengertian-pagerank","text":"Algoritma dari mesin telusur google yang mempunyai fungsi untuk menganalisis link dan memberikan nomor atau peringkat ke setiap halaman web yang ada di internet inilah yang disebut pagerank . Pagerank sebuah halaman web dihitung dari 0-10, semakin besar pagerank yang dimiliki sebuah halaman web maka semakin besar peluang untuk mendapatkan posisi paling atas pada hasil penelusuran google . Namun, peluang untuk mendapatkan posisi paling atas di mesin pencari google ini tidak memiliki pengaruh yang besar seperti dulu, karena sekarang google sudah menerapkan beberapa algoritma baru yang berfungsi untuk menentukan halaman web mana yang harus menempati urutan pertama dan seterusnya.","title":"Pengertian Pagerank"},{"location":"Pagerank/#fungsi-pagerank","text":"Fungsi utama dari pagerank yaitu menganalisis berbagai link yang masuk ( backlink ) yang nantinya akan dihitung berapa jumlah link yang masuk ( inbound ) dan link yang keluar ( outbound ) dari sebuah halaman web tersebut, lalu akan menghasilkan pagerank yang kita dapatkan. Fungsi lainnya dari pagerank yaitu menentukan situs web mana yang lebih populer atau lebih penting dimulai dari skala 10 (yang paling tinggi) hingga skala 0 (yang paling rendah). Semakin tinggi pagerank yang dimiliki sebuah web maka semakin populer lah halaman web tersebut, hal ini dikarenakan dengan tingginya pagerank yang dimiliki sebuah web itu berarti bahwa situs web tersebut banyak ditautkan oleh situs web lainnya.","title":"Fungsi Pagerank"},{"location":"Pagerank/#konsep-dasar-pagerank","text":"Pagerank memiliki konsep dasar yang sama dengan link popularity, tetapi tidak hanya memperhitungkan \u201cjumlah\u201d inbound link dan outbound link. Pendekatan yang digunakan adalah sebuah halaman akan dianggap penting jika halaman lain memiliki link ke halaman tersebut. Sebuah halaman juga akan menjadi semakin penting jika halaman lain yang memiliki rangking ( pagerank ) tinggi mengacu ke halaman tersebut. Sistem pagerank dipandang sangat tinggi dalam hal obyektifitas dan akurasi relefansi pencariannya. Mereka menggunakan struktur linknya yang kuat sebagai indikator dari nilai sebuah halaman. Google menginterpretasikan link dari halaman A ke halaman B sebagai sebuah pilihan atau vote, oleh halaman A, untuk halaman B.Tetapi, google melihat lebih dari sekedar jumlah pilihan atau link yang diterima oleh sesuatu halaman, Google juga menganalisis halaman yang memberi pilihan tersebut. Pilihan yang diberikan oleh halaman yang penting dimana halaman tersebut berbobot lebih tinggi sehingga dapat membantu membuat halaman lain menjadi penting. Situs yang penting dan berkualitas tinggi akan memperoleh lebih banyak pagerank dan akan diingat oleh google pada tiap-tiap pencarian. Tetapi, halaman yang penting tetap saja tidak berguna bila ternyata tidak sesuai dengan keinginan seseorang. Oleh karena itu, google mengkombinasikan pagerank dengan teknik pembandingan teks untuk mencari halaman web yang penting dan relevan dengan apa yang dicari oleh seseorang. google lebih dari sekedar menghitung jumlah keberadaan kata dalam sebuah halaman dan memeriksa segala aspek dari isi halaman termasuk halaman-halaman yang terkait untuk memutuskan apakah halaman tersebut cocok dengan apa yang dicari seseorang. Yang dimaksud faktor peredam ialah faktor yang menyertakan pengguna dalam kalkulasi.","title":"Konsep Dasar Pagerank"},{"location":"Pagerank/#cara-menghitung-pagerank","text":"Dari pendekatan yang sudah dijelaskan sebelumnya mengenai konsep pagerank , algoritma pagerank dapat dirumuskan seperti di bawah ini : Algoritma awal : $$ PR(A) = (1-d) + d ( ( PR(T1)/C (T1) )+ . . .+ ( PR(Tn) / C(Tn) ) ) $$ Salah satu algoritma lain yang dipublikasikan : $$ PR(A) = (1-d) / N + d ( (PR(T1) / C(T1) ) + \u2026 + ( PR(Tn) / C (Tn) ) ) $$ Keterangan dari algoritma diatas adalah : PR(A) adalah pagerank halaman A PR(T1) adalah pagerank halaman T1 yang mengacu ke halaman A C(T1) adalah jumlah link keluar ( outbound link ) pada halaman T1 d adalah damping factor yang bisa diberi antara 0 dan 1. N adalah jumlah keseluruhan halaman web (yang terindeks oleh Google) Dari algoritma di atas dapat dilihat bahwa pagerank ditentukan untuk setiap halaman anda bukan keseluruhan situs web. Pagerank sebuah halaman ditentukan dari pagerank halaman yang mengacu kepadanya yang juga menjalani proses penentuan pagerank dengan cara yang sama, jadi proses ini akan berulang sampai ditemukan hasil yang tepat. Akan tetapi pagerank halaman A tidak langsung diberikan kepada halaman yang dituju, melainkan sebelumnya dibagi dengan jumlah link yang ada pada halaman T1 (outbound link), dan pagerank itu akan dibagi rata kepada setiap link yang ada pada halaman tersebut. Demikian juga dengan setiap halaman lain \u201cTn\u201d yang mengacu ke halaman \u201cA\u201d. Setelah semua pagerank yang didapat dari halaman-halaman lain yang mengacu ke halaman \u201cA\u201d dijumlahkan, nilai itu kemudian dikalikan dengan damping factor yang bernilai antara 0 sampai 1. Hal ini dilakukan agar tidak semua nilai pagerank halaman T didistribusikan ke halaman A. Berapa nilai PR yang benar adalah hak hitung dari google Dari algoritma di atas dapat dilihat bahwa pagerank ditentukan untuk setiap halaman bukan keseluruhan situs web. Pagerank sebuah halaman ditentukan dari pagerank halaman yang mengacu kepadanya yang juga menjalani proses penentuan pagerank dengan cara yang sama, jadi proses ini akan berulang sampai ditemukan hasil yang tepat. Akan tetapi pagerank halaman A tidak langsung diberikan kepada halaman yang dituju, akan tetapi sebelumnya dibagi dengan jumlah link yang ada pada halaman T1 (outbound link), dan pagerank itu akan dibagi rata kepada setiap link yang ada pada halaman tersebut. Demikian juga dengan setiap halaman lain \u201cTn\u201d yang mengacu ke halaman \u201cA\u201d. Setelah semua pagerank yang didapat dari halaman-halaman lain yang mengacu ke halaman \u201cA\u201d dijumlahkan, nilai itu kemudian dikalikan dengan damping factor yang bernilai antara 0 sampai 1. Hal ini dilakukan agar tidak keseluruhan nilai pagerank halaman T didistribusikan ke halaman A.","title":"Cara Menghitung Pagerank"},{"location":"Pagerank/#kalkulasi-pagerank","text":"Nilai pagerank tinggi masih merupakan faktor umum untuk menilai otoritas sebuah website. Berikut ini merupakan tabel perhitungan pagerank secara manual : PR Link for PR3 Link for PR4 Link for PR5 Link for PR6 Link for PR7 Link for PR8 PR 1 555 3,005 16,803 92,414 508,277 2,795,522 PR 2 101 555 3,005 16,803 92,414 508,277 PR 3 18.5 101 555 3,005 16,803 92,414 PR 4 3.5 18.5 101 555 3,005 16,803 PR 5 1 3.5 18.5 101 555 3,005 PR 6 0.5 1 3.5 18.5 101 555 PR 7 0.5 0.5 1 3.5 18.5 101 PR 8 0.5 0.5 0.5 1 3.5 18.5 PR 9 0.5 0.5 0.5 0.5 1 3.5 PR 10 0.5 0.5 0.5 0.5 0.5 1 Dasar pagerank cukup mudah, yang perlu diketahui dalam perhitungan PR ( pagerank ) adalah: Berapa incoming link ? Berapa banyak total halaman lain terhubung ke website anda ? Bagaimana PR ( pagerank ) halaman website yang terlink ke website anda ? Contoh kalkulasi pagerank sederhana: Mari kita berasumsi bahwa seluruh web hanya terdiri atas empat website A, B, C, dan D, masing-masing memiliki nilai pagerank yaitu \u201c*1\u201d. Jumlahnya sama dengan jumlah website. Website B, C dan D masing-masing memiliki sebuah link ke website A dan tidak ada link lainnya. Apabila faktor peredam diabaikan, hasilnya adalah rumus : PR(A) = 1/1 + 1/1 dan 1/1, *pagerank website adalah 3. Contoh yang lebih rumit: Website A memiliki link ke website B dan C. B hanya memiliki sebuah link ke A. C memiliki link ke A, B dan D. D hanya memiliki link ke B. Rumus untuk A akan menjadi PR (A) = 1/1 + \u2153. Link dari B bernilai 1, sementara dari C hanya \u2153 dengan jumlah link 3, hasilnya adalah 1,33. Untuk B : PR(B) = \u00bd + \u2153 + 1/1 hasilnya 1, 83 Untuk C : PR(C) = \u00bd hasilnya 0,5 Untuk D : PR(D) = \u2153 hasilnya 0,33 Jumlah pagerank website A, B, C dan D seharusnya sama dengan jumlah website 1,33 + 1,83 + 0,5 + 0,33 = 3,99. Kekurangan 0,1 disebabkan oleh adanya pembulatan. Dalam kalkulasi ini masih ada yang kurang. Pagerank setiap website tidak disertakan. Contoh berikutnya adalah website b. Apabila kalkulasi disesuaikan dengan pagerank yang didapat dari langkah pertama . PR(B) = \u00bd +\u2153 +1/1. Didapat term berikut : PR(B) = 1,33/2 + 0,5/3 +0,33/1 hasilnya adalah 1,62. Tentu saja kalkulasi baru pagerank website B mengubah pagerank website A, C dan D. Nilai baru website D kembali mengubah nilai website B.","title":"Kalkulasi pagerank"},{"location":"Pagerank/#code-perhitungan-pagerank","text":"Berikut ini merupakan code untuk menghitung pagerank : damping = 0.85 max_iterr = 100 error_toleransi = 0.01 pr = nx.pagerank(g, alpha - damping, max_iter1=max_iterr, tol1=error_toleransi) Pada penjelasan diatas telah dijabarkan bagaimana perhitungan manual pagerank. Sedangkan untuk code diatas merupakan perhitungan pagerank dengan library yang terdapat pada python yaitu networkx. Dalam networkx memiliki module pagerank yang memiliki parameter diatas yaitu g yang merupakan variabel yang nantinya menampung graph, alpha beridi nilai damping dengan default 0.85, max_iter untuk nilai maksimum iterasi yang diizinkan, terakhir yatu tol untuk menampung nilai minimal eror toleransi. Code untuk menampung hasil perhitungan ke dalam list : print(\"keterangan node:\") nodelist = g.nodes label= {} data=[] for i, key in enumerate(nodelist): data.append((pr[key], key)) label[key]=i Langkah selanjutnya yaitu mengurutkan nilai pagerank. Code untuk mengurutkan pagerank adalah sebagai berikut : urut = data.copy() for x in range(len(urut)): for y in range(len(urut)): if urut[x][0] > urut[y][0]: urut[x], urut[y] = urut[y], urut[x] urut = pd.DataFrame(data, None, (\"Pagerank\", \"Node\"))","title":"Code Perhitungan Pagerank :"},{"location":"Pagerank/#graph","text":"Selanjutnya yaitu membentuk garph, graph disini berfungsi agar mendapatkan visualisasi yang lebih terstruktur dari hasil pagerank. Code membentuk graph : g = nx.from_pandas_edgelist(edgeListFrame, \"From\", \"To\", None, nx.DiGraph()) pos = nx.spring_layout(g) plt.title('Link Graph') nx.draw(g, pos) nx.draw_networkx_labels(g, pos, label, font_color=\"w\") plt.axis(\"off\") plt.show()","title":"Graph"},{"location":"Pentingnya_Preprocessing/","text":"\u00b6 Pentingnya Preprocessing \u00b6 Mengapa perlu data preprocessing? Data mentah yang ada sebagian besar kotor Tidak komplet Berisi data yang hilang / kosong Kekurangan atribut yang sesuai Hanya berisi data aggregate Banyak \u201cnoise\u201d Berisi data yang Outlier Berisi error Tidak konsisten Berisi nilai yang berbeda dalam suatu kode atau nama Mengapa Data Preprocessing Penting ? Data yang tidak berkualitas, akan menghasilkan kualitas mining yang tidak baik pula. Data Preprocessing, cleaning, dan transformasi merupakan pekerjaan mayoritas dalam aplikasi data mining (90%).","title":"Pentingnya Preprocessing"},{"location":"Pentingnya_Preprocessing/#pentingnya-preprocessing","text":"Mengapa perlu data preprocessing? Data mentah yang ada sebagian besar kotor Tidak komplet Berisi data yang hilang / kosong Kekurangan atribut yang sesuai Hanya berisi data aggregate Banyak \u201cnoise\u201d Berisi data yang Outlier Berisi error Tidak konsisten Berisi nilai yang berbeda dalam suatu kode atau nama Mengapa Data Preprocessing Penting ? Data yang tidak berkualitas, akan menghasilkan kualitas mining yang tidak baik pula. Data Preprocessing, cleaning, dan transformasi merupakan pekerjaan mayoritas dalam aplikasi data mining (90%).","title":"Pentingnya Preprocessing"},{"location":"Preprocessing/","text":"Preprocessing \u00b6 Tahapan selanjutnya adalah Preprocessing / Cleaning. Proses cleaning / preprocessing mencakup antara lain membuang duplikasi data, memeriksa data yang inkonsisten, dan memperbaiki kesalahan pada data. Pada halaman ini saya akan membahas salah satu bentuk dari preprocessing yaitu seleksi fitur. Apa fungsi dari Seleksi Fitur ? Seleksi Fitur \u00b6 Tugas utama seleksi fitur adalah menentukan istilah-istilah yang layak dijadikan term index atau dengan kata lain membuang (menghilangkan) istilah-istilah yang tidak mungkin dijadikan indeks. Terdapat beberapa cara yang dapat dilakukan untuk mengeliminasi istilah-istilah yang kurang merepresentasikan dokumen tersebut, diantaranya adalah menghilangkan istilah-istilah yang sering muncul pada berbagai dokumen. Istilah-istilah yang sering muncul pada berbagai dokumen biasanya adalah istilah-istilah yang tidak mempunyai arti terhadap dokumen tersebut, jika istilah ini dihilangkan, tidak mengurangi makna dokumennya. Kata sambung seperti dan, atau dan juga merupakan contoh dari kaat sambung. Disamping istilah yang sering muncul, istilah-istilah yang jarang muncul, atau hanya muncul satu atau dua kali pada satu dokumen juga merupakan kandidat istilah yang dapat dihilangkan. Pada halaman ini, seleksi fitur saya menggunakan metode Pearson Correlation. Apa yang dimaksud metode Pearson Correlation pada seleksi fitur ? Pearson Correlation \u00b6 Korelasi Pearson merupakan salah satu ukuran korelasi yang digunakan untuk mengukur kekuatan dan arah hubungan linier dari dua veriabel. Dua variabel dikatakan berkorelasi apabila perubahan salah satu variabel disertai dengan perubahan variabel lainnya, baik dalam arah yang sama ataupun arah yang sebaliknya. Harus diingat bahwa nilai koefisien korelasi yang kecil (tidak signifikan) bukan berarti kedua variabel tersebut tidak saling berhubungan . Mungkin saja dua variabel mempunyai keeratan hubungan yang kuat namun nilai koefisien korelasinya mendekati nol, misalnya pada kasus hubungan non linier . Dengan demikian, koefisien korelasi hanya mengukur kekuatan hubungan linier dan tidak pada hubungan non linier**. **Harus diingat pula bahwa adanya hubungan linier yang kuat di antara variabel tidak selalu berarti ada hubungan kausalitas, sebab-akibat. Rumus dari Pearson Correlation adalah sebagai berikut : Rumus yang dipergunakan untuk menghitung pearson correlation adalah sebagai berikut : (Rumus ini disebut juga dengan Pearson Product Moment) r = n\u03a3xy \u2013 (\u03a3x) (\u03a3y) . \u221a{n\u03a3x\u00b2 \u2013 (\u03a3x)\u00b2} {n\u03a3y2 \u2013 (\u03a3y)2} Dimana : n = Banyaknya Pasangan data X dan Y \u03a3x = Total Jumlah dari Variabel X \u03a3y = Total Jumlah dari Variabel Y \u03a3x2= Kuadrat dari Total Jumlah Variabel X \u03a3y2= Kuadrat dari Total Jumlah Variabel Y \u03a3xy= Hasil Perkalian dari Total Jumlah Variabel X dan Variabel Y Code untuk Pearson Correlation : def pearsonCalculate(data, u,v): \"i, j is an index\" atas=0; bawah_kiri=0; bawah_kanan = 0 for k in range(len(data)): atas += (data[k,u] - meanFitur[u]) * (data[k,v] - meanFitur[v]) bawah_kiri += (data[k,u] - meanFitur[u])**2 bawah_kanan += (data[k,v] - meanFitur[v])**2 bawah_kiri = bawah_kiri ** 0.5 bawah_kanan = bawah_kanan ** 0.5 return atas/(bawah_kiri * bawah_kanan) def meanF(data): meanFitur=[] for i in range(len(data[0])): meanFitur.append(sum(data[:,i])/len(data)) return np.array(meanFitur) def seleksiFiturPearson(katadasar, data, threshold): global meanFitur meanFitur = meanF(data) u=0 while u < len(data[0]): dataBaru=data[:, :u+1] meanBaru=meanFitur[:u+1] kataBaru=katadasar[:u+1] v = u while v < len(data[0]): if u != v: value = pearsonCalculate(data, u,v) if value < threshold: dataBaru = np.hstack((dataBaru, data[:, v].reshape(data.shape[0],1))) meanBaru = np.hstack((meanBaru, meanFitur[v])) kataBaru = np.hstack((kataBaru, katadasar[v])) v+=1 data = dataBaru meanFitur=meanBaru katadasar=kataBaru if u%50 == 0 : print(\"seleksi : \", u, data.shape) u+=1 return data,kataBaru Pemanggilan function pada code diatas hanya perlu memanggil function SeleksiFiturPearson() dengan parameter data yang akan diseleksi (type data harus numpy array) dan batas nilai korelasi yang akan digunakan untuk membuang fitur yang mirip. Misalnya seperti ini: tf = numpy.array(tfidf) batas = 0.8 fiturBaru = seleksiFiturPearson(tfidf, batas) Output yang dihasilkan berupa fitur - fitur yang telah diseleksi","title":"Preprocessing"},{"location":"Preprocessing/#preprocessing","text":"Tahapan selanjutnya adalah Preprocessing / Cleaning. Proses cleaning / preprocessing mencakup antara lain membuang duplikasi data, memeriksa data yang inkonsisten, dan memperbaiki kesalahan pada data. Pada halaman ini saya akan membahas salah satu bentuk dari preprocessing yaitu seleksi fitur. Apa fungsi dari Seleksi Fitur ?","title":"Preprocessing"},{"location":"Preprocessing/#seleksi-fitur","text":"Tugas utama seleksi fitur adalah menentukan istilah-istilah yang layak dijadikan term index atau dengan kata lain membuang (menghilangkan) istilah-istilah yang tidak mungkin dijadikan indeks. Terdapat beberapa cara yang dapat dilakukan untuk mengeliminasi istilah-istilah yang kurang merepresentasikan dokumen tersebut, diantaranya adalah menghilangkan istilah-istilah yang sering muncul pada berbagai dokumen. Istilah-istilah yang sering muncul pada berbagai dokumen biasanya adalah istilah-istilah yang tidak mempunyai arti terhadap dokumen tersebut, jika istilah ini dihilangkan, tidak mengurangi makna dokumennya. Kata sambung seperti dan, atau dan juga merupakan contoh dari kaat sambung. Disamping istilah yang sering muncul, istilah-istilah yang jarang muncul, atau hanya muncul satu atau dua kali pada satu dokumen juga merupakan kandidat istilah yang dapat dihilangkan. Pada halaman ini, seleksi fitur saya menggunakan metode Pearson Correlation. Apa yang dimaksud metode Pearson Correlation pada seleksi fitur ?","title":"Seleksi Fitur"},{"location":"Preprocessing/#pearson-correlation","text":"Korelasi Pearson merupakan salah satu ukuran korelasi yang digunakan untuk mengukur kekuatan dan arah hubungan linier dari dua veriabel. Dua variabel dikatakan berkorelasi apabila perubahan salah satu variabel disertai dengan perubahan variabel lainnya, baik dalam arah yang sama ataupun arah yang sebaliknya. Harus diingat bahwa nilai koefisien korelasi yang kecil (tidak signifikan) bukan berarti kedua variabel tersebut tidak saling berhubungan . Mungkin saja dua variabel mempunyai keeratan hubungan yang kuat namun nilai koefisien korelasinya mendekati nol, misalnya pada kasus hubungan non linier . Dengan demikian, koefisien korelasi hanya mengukur kekuatan hubungan linier dan tidak pada hubungan non linier**. **Harus diingat pula bahwa adanya hubungan linier yang kuat di antara variabel tidak selalu berarti ada hubungan kausalitas, sebab-akibat. Rumus dari Pearson Correlation adalah sebagai berikut : Rumus yang dipergunakan untuk menghitung pearson correlation adalah sebagai berikut : (Rumus ini disebut juga dengan Pearson Product Moment) r = n\u03a3xy \u2013 (\u03a3x) (\u03a3y) . \u221a{n\u03a3x\u00b2 \u2013 (\u03a3x)\u00b2} {n\u03a3y2 \u2013 (\u03a3y)2} Dimana : n = Banyaknya Pasangan data X dan Y \u03a3x = Total Jumlah dari Variabel X \u03a3y = Total Jumlah dari Variabel Y \u03a3x2= Kuadrat dari Total Jumlah Variabel X \u03a3y2= Kuadrat dari Total Jumlah Variabel Y \u03a3xy= Hasil Perkalian dari Total Jumlah Variabel X dan Variabel Y Code untuk Pearson Correlation : def pearsonCalculate(data, u,v): \"i, j is an index\" atas=0; bawah_kiri=0; bawah_kanan = 0 for k in range(len(data)): atas += (data[k,u] - meanFitur[u]) * (data[k,v] - meanFitur[v]) bawah_kiri += (data[k,u] - meanFitur[u])**2 bawah_kanan += (data[k,v] - meanFitur[v])**2 bawah_kiri = bawah_kiri ** 0.5 bawah_kanan = bawah_kanan ** 0.5 return atas/(bawah_kiri * bawah_kanan) def meanF(data): meanFitur=[] for i in range(len(data[0])): meanFitur.append(sum(data[:,i])/len(data)) return np.array(meanFitur) def seleksiFiturPearson(katadasar, data, threshold): global meanFitur meanFitur = meanF(data) u=0 while u < len(data[0]): dataBaru=data[:, :u+1] meanBaru=meanFitur[:u+1] kataBaru=katadasar[:u+1] v = u while v < len(data[0]): if u != v: value = pearsonCalculate(data, u,v) if value < threshold: dataBaru = np.hstack((dataBaru, data[:, v].reshape(data.shape[0],1))) meanBaru = np.hstack((meanBaru, meanFitur[v])) kataBaru = np.hstack((kataBaru, katadasar[v])) v+=1 data = dataBaru meanFitur=meanBaru katadasar=kataBaru if u%50 == 0 : print(\"seleksi : \", u, data.shape) u+=1 return data,kataBaru Pemanggilan function pada code diatas hanya perlu memanggil function SeleksiFiturPearson() dengan parameter data yang akan diseleksi (type data harus numpy array) dan batas nilai korelasi yang akan digunakan untuk membuang fitur yang mirip. Misalnya seperti ini: tf = numpy.array(tfidf) batas = 0.8 fiturBaru = seleksiFiturPearson(tfidf, batas) Output yang dihasilkan berupa fitur - fitur yang telah diseleksi","title":"Pearson Correlation"},{"location":"Referensi/","text":"Referensi \u00b6 Berikut ini terlampir referensi dari tahapan - tahapan yang telah dibuat : https://www.dictio.id/t/apa-yang-dimaksud-dengan-web-crawler/15116 https://scrapy.org/ https://www.octoparse.com/blog/how-to-build-a-web-crawler-from-scratch-a-guide-for-beginners https://www.laserfiche.com/support/webhelp/quickfields/8.0/en-us/content/text%20extraction.htm https://www.researchgate.net/publication/292286778_A_Effective_Preprocessing_for_Web_Usage_Mining https://pdfs.semanticscholar.org/39ea/984a452ed12992a8a494cc9e1df65545b2fa.pdf http://ilmukomputer.org/wp-content/uploads/2018/05/agus-k-means-clustering.pdf https://piptools.net/algoritma-fcm-fuzzy-c-means-clustering/","title":"Referensi"},{"location":"Referensi/#referensi","text":"Berikut ini terlampir referensi dari tahapan - tahapan yang telah dibuat : https://www.dictio.id/t/apa-yang-dimaksud-dengan-web-crawler/15116 https://scrapy.org/ https://www.octoparse.com/blog/how-to-build-a-web-crawler-from-scratch-a-guide-for-beginners https://www.laserfiche.com/support/webhelp/quickfields/8.0/en-us/content/text%20extraction.htm https://www.researchgate.net/publication/292286778_A_Effective_Preprocessing_for_Web_Usage_Mining https://pdfs.semanticscholar.org/39ea/984a452ed12992a8a494cc9e1df65545b2fa.pdf http://ilmukomputer.org/wp-content/uploads/2018/05/agus-k-means-clustering.pdf https://piptools.net/algoritma-fcm-fuzzy-c-means-clustering/","title":"Referensi"},{"location":"Referensipage/","text":"Referensi \u00b6 Berikut ini terlampir referensi dari tahapan yang telah dibuat : http://jim.stimednp.ac.id/wp-content/uploads/2014/03/IMPLEMENTASI-ALGORITMA-PAGERANK-PADA.pdf http://jurnal.uinsu.ac.id/index.php/zero/article/download/1458/1183 http://gembong.lecture.ub.ac.id/seri-tutorial-pagerank-kalkulasi-pagerank/","title":"Referensi"},{"location":"Referensipage/#referensi","text":"Berikut ini terlampir referensi dari tahapan yang telah dibuat : http://jim.stimednp.ac.id/wp-content/uploads/2014/03/IMPLEMENTASI-ALGORITMA-PAGERANK-PADA.pdf http://jurnal.uinsu.ac.id/index.php/zero/article/download/1458/1183 http://gembong.lecture.ub.ac.id/seri-tutorial-pagerank-kalkulasi-pagerank/","title":"Referensi"},{"location":"Selvie Akmalia/","text":"Crawling \u00b6 Berikut ini, saya akan membahas tutorial mengambil data (crawl) dari sebuah Website dan disimpan ke database. Sebelumnya, disini saya menggunakan bahasa pemrograman Python. Bagi kalian yang belum mempunyai Python, kalian bisa mendownload di internet lalu menginstallnya di laptop masing - masing. Dibawah ini adalah tahapan - tahapan untuk mengcrawl data : Pertama - tama download Python di internet (kalian bisa memilih ingin mendownload versi berapa), saya memilih mendownload versi 3.6 Setelah Python terdownload, waktunya menginstall di laptop kalian masing - masing Sebelum menjalankan Python, anda perlu menginstall library request dan beautifulsoup dengan cara : Membuka command prompt di laptop anda Ketikkan kode berikut : pip install bs4 pip install bs4 digunakan untuk menginstall beautifulsoup. pip install requests pip install request digunakan untuk menginstall requests Mengapa kita harus menginstall beautifulsoup dan requests ? Kita perlu menginstall beautifulsoup untuk mengubah objek file ke beautifulsoup, sedangkan requests untuk mengambil data dari internet Setelah requests dan beautifulsoup telah terinstall, kalian bisa membuka Python yang telah terinstall Ketikkan code dibawah ini : import requests from bs4 import BeautifulSoup import sqlite3 Code diatas digunakan untuk mengambil semua library yang telah terinstall Setelah mengimport seluruh library yang dibutuhkan, maka langkah selanjutnya adalah mengkoneksikan database ke file .db yang telah dibuat sebelumnya, codenya seperti dibawah ini : conn = sqlite3.connect('test.db') Melakukan pengecekan jika terdapat nama tabel yang sama, tabel yang telah saya buat namanya adalah \"Berita\", jadi jika terdapat tabel yang namanya juga \"Berita\"maka otomatis akan dihapus, codenya seperti dibawah ini : conn.execute('drop table if exists Berita') Membuat tabel dengan nama sesuai keinginan, tabel disini saya beri nama \"Berita\" dengan field \"Judul\" dan \"Isi\". Codenya seperti dibawah ini : conn.execute('''CREATE TABLE Berita (Judul TEXT NOT NULL, Isi TEXT NOT NULL);''') Membuat variabel yang isinya adalah link URL dari website yang ingin kita ambil datanya. Codenya seperti dibawah ini : src = \"https://www.anehdidunia.com/\" Link URL yang saya ambil adalah www.anehdidunia.com Setelah itu kita melakukan konversi pada link URL diatas dan mengubahnya ke objek beautifulsoup. Codenya seperti dibawah ini : page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') FIeld yang telah kita buat tadi yaitu \"Judul\" dan \"Isi\" akan mengambil data berdasarkan class yang terdapat pada Website yang kita ambil. Untuk mengetahui class tersebut, kita cukup memblok teks yang akan kita crawl lalu klik kanan, setelah itu klik \"Inspect\" maka akan muncul class yang ingin kita ambil. Codenya seperti dibawah ini : judul = soup.findAll(class_='post-title entry-title') isi = soup.findAll(class_='snippets') Dengan menggunakan perulangan for, kita akan membuat variabel baru yang bernama a dan b kemudian mendeklarasikan ke tipe data \"string\" dengan deklarasi \"%s\" dan \"%s\". Codenya seperti dibawah ini : for i in range(len(judul)): a = judul[i].getText() b = isi[i].getText() conn.execute('INSERT INTO Berita(Judul, Isi) VALUES (\"%s\", \"%s\")' %(a, b)); Membuat variabel yang isinya akan menampilkan seluruh isi yang terdapat pada tabel yang bernama \"Berita\". Codenya seperti dibawah ini : cursor = conn.execute(\"SELECT * from Berita\") Menampilkan baris yang telah diambil pada tabel \"Berita\". Codenya seperti dibawah ini : for row in cursor: print(row)","title":"Crawling"},{"location":"Selvie Akmalia/#crawling","text":"Berikut ini, saya akan membahas tutorial mengambil data (crawl) dari sebuah Website dan disimpan ke database. Sebelumnya, disini saya menggunakan bahasa pemrograman Python. Bagi kalian yang belum mempunyai Python, kalian bisa mendownload di internet lalu menginstallnya di laptop masing - masing. Dibawah ini adalah tahapan - tahapan untuk mengcrawl data : Pertama - tama download Python di internet (kalian bisa memilih ingin mendownload versi berapa), saya memilih mendownload versi 3.6 Setelah Python terdownload, waktunya menginstall di laptop kalian masing - masing Sebelum menjalankan Python, anda perlu menginstall library request dan beautifulsoup dengan cara : Membuka command prompt di laptop anda Ketikkan kode berikut : pip install bs4 pip install bs4 digunakan untuk menginstall beautifulsoup. pip install requests pip install request digunakan untuk menginstall requests Mengapa kita harus menginstall beautifulsoup dan requests ? Kita perlu menginstall beautifulsoup untuk mengubah objek file ke beautifulsoup, sedangkan requests untuk mengambil data dari internet Setelah requests dan beautifulsoup telah terinstall, kalian bisa membuka Python yang telah terinstall Ketikkan code dibawah ini : import requests from bs4 import BeautifulSoup import sqlite3 Code diatas digunakan untuk mengambil semua library yang telah terinstall Setelah mengimport seluruh library yang dibutuhkan, maka langkah selanjutnya adalah mengkoneksikan database ke file .db yang telah dibuat sebelumnya, codenya seperti dibawah ini : conn = sqlite3.connect('test.db') Melakukan pengecekan jika terdapat nama tabel yang sama, tabel yang telah saya buat namanya adalah \"Berita\", jadi jika terdapat tabel yang namanya juga \"Berita\"maka otomatis akan dihapus, codenya seperti dibawah ini : conn.execute('drop table if exists Berita') Membuat tabel dengan nama sesuai keinginan, tabel disini saya beri nama \"Berita\" dengan field \"Judul\" dan \"Isi\". Codenya seperti dibawah ini : conn.execute('''CREATE TABLE Berita (Judul TEXT NOT NULL, Isi TEXT NOT NULL);''') Membuat variabel yang isinya adalah link URL dari website yang ingin kita ambil datanya. Codenya seperti dibawah ini : src = \"https://www.anehdidunia.com/\" Link URL yang saya ambil adalah www.anehdidunia.com Setelah itu kita melakukan konversi pada link URL diatas dan mengubahnya ke objek beautifulsoup. Codenya seperti dibawah ini : page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') FIeld yang telah kita buat tadi yaitu \"Judul\" dan \"Isi\" akan mengambil data berdasarkan class yang terdapat pada Website yang kita ambil. Untuk mengetahui class tersebut, kita cukup memblok teks yang akan kita crawl lalu klik kanan, setelah itu klik \"Inspect\" maka akan muncul class yang ingin kita ambil. Codenya seperti dibawah ini : judul = soup.findAll(class_='post-title entry-title') isi = soup.findAll(class_='snippets') Dengan menggunakan perulangan for, kita akan membuat variabel baru yang bernama a dan b kemudian mendeklarasikan ke tipe data \"string\" dengan deklarasi \"%s\" dan \"%s\". Codenya seperti dibawah ini : for i in range(len(judul)): a = judul[i].getText() b = isi[i].getText() conn.execute('INSERT INTO Berita(Judul, Isi) VALUES (\"%s\", \"%s\")' %(a, b)); Membuat variabel yang isinya akan menampilkan seluruh isi yang terdapat pada tabel yang bernama \"Berita\". Codenya seperti dibawah ini : cursor = conn.execute(\"SELECT * from Berita\") Menampilkan baris yang telah diambil pada tabel \"Berita\". Codenya seperti dibawah ini : for row in cursor: print(row)","title":"Crawling"},{"location":"Webstructuremining/","text":"Web Structure Mining \u00b6 Web struncture mining dikenal juga sebagai web log mining adalah teknik yang digunakan untuk menemukan struktur link dari hyperlink dan membangun rangkuman website dan halaman web. Salah satu manfaatnya adlah untuk menentukan pagerank pada suatu halaman web. Web structure mining digunakan untuk mengidentifikasi otoritatif halaman dan hub, yang merupakan landasan dari algoritma page-rank kontemporer yang penting bagi mesin pencari populer seperti Google dan Yahoo!. Analisis link sangat penting dalam memahami hubungan timbal balik antara sejumlah besar halaman web, yang mengarah ke pemahaman yang lebih baik dari komunitas web tertentu, klan, atau klik. Tools & Environment \u00b6 Python 3.6 Library untuk crawl : BeautifulSoup4, requests Library untuk menampung serta menghitung pagerank : pa, networks Library menampilkan graph : matplotlib Website target : https://www.anehdidunia.com Proses pertama yang dilakukan adalah crawling data. Pertama-tama koneksikan ke library terlebih dahulu : import requests from bs4 import BeautifulSoup Selanjutnya adalah membuat fungsi mengambil data, dan fungsi filter link. Code nya sebagai berikut : def getAllLinks(src): try: ind = src.find(':')+3 url = src[ind:] page = requests.get(src) # Mengubah html ke object beautiful soup soup = BeautifulSoup(page.content, 'html.parser') tags = soup.findAll(\"a\") links = [] for tag in tags: try: link = tag['href'] if not link in links and 'http' in link: links.append(link) except KeyError: pass return links def simplifiedURL(url): # cek 1 : www if \"www.\" in url: ind = url.index(\"www.\")+4 url = url[ind:] # cek 2 : tanda / di akhir if url[-1] == \"/\": url = url[:-1] # cek 3 : parts = url.split(\"/\") url = '' for i in range(3) : url += parts[i]+\"/\" return url Code diatas berfungsi untuk mengambil data yang memiliki hyperlink dalam tag nya. Hal pertama yang dilakukan yaitu mengambil seluruh data yang memiliki tag a didalamnya, lalu melakukan filter dengan cara mengambil tag a yang memiliki atribut href kemudian langkah selanjutnya yaitu mengambil isi dari atribut href yang nantinya akan ditampung dalam list. Untuk code fungsi filter link terdapat tiga kondisi, untuk kondisi pertama mempunyai fungsi mengubah link menjadi http. Kondisi kedua mempunyai fungsi mengecek ada tidaknya garis miring setelah link yang akan di crawl, jika terdapat garis link maka link tersebut akan diambil dari awal hingga posisi terakhir. Sedangkan untuk kondisi ketiga mempunyai fungsi untuk mengambil link utama dari website. Code untuk fungsi crawl : def crawl(url, max_deep, show=False, deep=0): # returnnya ada di edgelist, global edgelist # menyamakan format url, agar tidak ada url yg dobel url = simplifiedURL(url) # crawl semua link links = getAllLinks(url) # menambah counter kedalaman deep += 1 #menampilkan proses if show: if deep == 1: print(\"(%d)%s\" %(len(links),url)) else: print(\"|\", end=\"\") for i in range(deep-1): print(\"--\", end=\"\") print(\"(%d)%s\" %(len(links),url)) for link in links: # Membentuk format jalan (edge => (dari, ke)) edge = (url,link) # Mengecek jalan, apabila belum dicatat, maka dimasukkan ke list if not edge in edgelist: edgelist.append(edge) # Cek kedalaman, jika belum sampai terakhir, maka crawling. if (deep != max_deep): crawl(link, max_deep, show, deep) Telah terdapat dua fungsi yaitu fungsi mengambil data dan fungsi filter. Dari dua fungsi tersebut nantinya akan digabungkan dengan memanggil kedua fungsi didalam fungsi crawl seperti code diatas. Fungsi ini akan dipanggil lagi pada main program. Code memanggil fungsi pada main adalah sebagai berikut : root = \"https://www.anehdidunia.com/\" nodelist = [root] done = [root] edgelist = [] #crawl tampilkan = True crawl(root, 3, show=tampilkan) if tampilkan: for i in range(10): print(\"\")","title":"Home"},{"location":"Webstructuremining/#web-structure-mining","text":"Web struncture mining dikenal juga sebagai web log mining adalah teknik yang digunakan untuk menemukan struktur link dari hyperlink dan membangun rangkuman website dan halaman web. Salah satu manfaatnya adlah untuk menentukan pagerank pada suatu halaman web. Web structure mining digunakan untuk mengidentifikasi otoritatif halaman dan hub, yang merupakan landasan dari algoritma page-rank kontemporer yang penting bagi mesin pencari populer seperti Google dan Yahoo!. Analisis link sangat penting dalam memahami hubungan timbal balik antara sejumlah besar halaman web, yang mengarah ke pemahaman yang lebih baik dari komunitas web tertentu, klan, atau klik.","title":"Web Structure Mining"},{"location":"Webstructuremining/#tools-environment","text":"Python 3.6 Library untuk crawl : BeautifulSoup4, requests Library untuk menampung serta menghitung pagerank : pa, networks Library menampilkan graph : matplotlib Website target : https://www.anehdidunia.com Proses pertama yang dilakukan adalah crawling data. Pertama-tama koneksikan ke library terlebih dahulu : import requests from bs4 import BeautifulSoup Selanjutnya adalah membuat fungsi mengambil data, dan fungsi filter link. Code nya sebagai berikut : def getAllLinks(src): try: ind = src.find(':')+3 url = src[ind:] page = requests.get(src) # Mengubah html ke object beautiful soup soup = BeautifulSoup(page.content, 'html.parser') tags = soup.findAll(\"a\") links = [] for tag in tags: try: link = tag['href'] if not link in links and 'http' in link: links.append(link) except KeyError: pass return links def simplifiedURL(url): # cek 1 : www if \"www.\" in url: ind = url.index(\"www.\")+4 url = url[ind:] # cek 2 : tanda / di akhir if url[-1] == \"/\": url = url[:-1] # cek 3 : parts = url.split(\"/\") url = '' for i in range(3) : url += parts[i]+\"/\" return url Code diatas berfungsi untuk mengambil data yang memiliki hyperlink dalam tag nya. Hal pertama yang dilakukan yaitu mengambil seluruh data yang memiliki tag a didalamnya, lalu melakukan filter dengan cara mengambil tag a yang memiliki atribut href kemudian langkah selanjutnya yaitu mengambil isi dari atribut href yang nantinya akan ditampung dalam list. Untuk code fungsi filter link terdapat tiga kondisi, untuk kondisi pertama mempunyai fungsi mengubah link menjadi http. Kondisi kedua mempunyai fungsi mengecek ada tidaknya garis miring setelah link yang akan di crawl, jika terdapat garis link maka link tersebut akan diambil dari awal hingga posisi terakhir. Sedangkan untuk kondisi ketiga mempunyai fungsi untuk mengambil link utama dari website. Code untuk fungsi crawl : def crawl(url, max_deep, show=False, deep=0): # returnnya ada di edgelist, global edgelist # menyamakan format url, agar tidak ada url yg dobel url = simplifiedURL(url) # crawl semua link links = getAllLinks(url) # menambah counter kedalaman deep += 1 #menampilkan proses if show: if deep == 1: print(\"(%d)%s\" %(len(links),url)) else: print(\"|\", end=\"\") for i in range(deep-1): print(\"--\", end=\"\") print(\"(%d)%s\" %(len(links),url)) for link in links: # Membentuk format jalan (edge => (dari, ke)) edge = (url,link) # Mengecek jalan, apabila belum dicatat, maka dimasukkan ke list if not edge in edgelist: edgelist.append(edge) # Cek kedalaman, jika belum sampai terakhir, maka crawling. if (deep != max_deep): crawl(link, max_deep, show, deep) Telah terdapat dua fungsi yaitu fungsi mengambil data dan fungsi filter. Dari dua fungsi tersebut nantinya akan digabungkan dengan memanggil kedua fungsi didalam fungsi crawl seperti code diatas. Fungsi ini akan dipanggil lagi pada main program. Code memanggil fungsi pada main adalah sebagai berikut : root = \"https://www.anehdidunia.com/\" nodelist = [root] done = [root] edgelist = [] #crawl tampilkan = True crawl(root, 3, show=tampilkan) if tampilkan: for i in range(10): print(\"\")","title":"Tools &amp; Environment"},{"location":"carakerja_web_crawler/","text":"Cara Kerja Web Crawler \u00b6 Web crawler atau yang dikenal juga dengan istilah web spider bertugas untuk mengumpulkan semua informasi yang ada di dalam halaman web. Web crawler bekerja secara otomatis dengan cara memberikan sejumlah alamat website untuk dikunjungi serta menyimpan semua informasi yang terkandung didalamnya. Setiap kali web crawler mengunjungi sebuah website, maka web crawler tersebut akan mendata semua link yang ada dihalaman yang dikunjunginya itu untuk kemudian di kunjungi lagi satu persatu. Proses web crawler dalam mengunjungi setiap dokumen web disebut dengan web crawling atau spidering. Beberapa websites, khususnya yang berhubungan dengan pencarian menggunakan proses spidering untuk memperbaharui data data mereka. Web crawler biasa digunakan untuk membuat salinan secara sebagian atau keseluruhan halaman web yang telah dikunjunginya agar dapat di proses lebih lanjut oleh system pengindexan. Crawler dapat juga digunakan untuk proses pemeliharaan sebuah website, seperti memvalidasi kode html sebuah web, dan crawler juga digunakan untuk memperoleh data yang khusus seperti mengumpulkan alamat e-mail. Web crawler termasuk kedalam bagian software agent atau yang lebih dikenal dengan istilah program bot. Secara umum crawler memulai prosesnya dengan memberikan daftar sejumlah alamat website untuk dikunjungi, disebut sebagai seeds. Setiap kali sebuah halaman web dikunjungi, crawler akan mencari alamat yang lain yang terdapat didalamnya dan menambahkan kedalam daftar seeds sebelumnya. Secara umum crawler memulai prosesnya dengan memberikan daftar sejumlah alamat website untuk dikunjungi, disebut sebagai seeds. Setiap kali sebuah halaman web dikunjungi, crawler akan mencari alamat yang lain yang terdapat didalamnya dan menambahkan kedalam daftar seeds sebelumnya. Dalam melakukan prosesnya, web crawler juga mempunyai beberapa persoalan yang harus mampu di atasinya. Permasalahan tersebut mencakup : Halaman mana yang harus dikunjungi terlebih dahulu. Aturan dalam proses mengunjungi kembali sebuah halaman. Performansi, mencakup banyaknya halaman yang harus dikunjungi. Aturan dalam setiap kunjungan agar server yang dikunjungi tidak kelebihan beban. Kegagalan, mencakup tidak tersedianya halaman yang dikunjungi, server down, timeout, maupun jebakan yang sengaja dibuat oleh webmaster. Seberapa jauh kedalaman sebuah website yang akan dikunjungi. Hal yang tak kalah pentingnya adalah kemampuan web crawler untuk mengikuti Perkembangan teknologi web, dimana setiap kali teknologi baru muncul, web crawler harus dapat menyesuaikan diri agar dapat mengunjungi halaman web yang menggunakan teknologi baru tersebut. Proses sebuah web crawler untuk mendata link \u2013 link yang terdapat didalam sebuah halaman web menggunakan pendekatan regular expression. Crawler akan menelurusi setiap karakter yang ada untuk menemukan hyperlink tag html. Setiap hyperlink tag yang ditemukan diperiksa lebih lanjut apakah tag tersebut mengandung atribut nofollow rel, jika tidak ada maka diambil nilai yang terdapat didalam attribute href yang merupakan sebuah link baru. Setelah proses crawler selesai di lanjutkan dengan indexing system yang bertugas untuk menganalisa halaman web yang telah tersimpan sebelumnya dengan cara mengindeks setiap kemungkinan term yang terdapat di dalamnnya. Data term yang ditemukan disimpan dalam sebuah database indeks untuk digunakan dalam pencarian selanjutnya. Indexing system mengumpulkan, memilah dan menyimpan data untuk memberikan kemudahan dalam pengaksesan informasi secara tepat dan akurat. Proses pengolahan halaman web agar dapat digunakan untuk proses pencarian berikutnya dinakamakan web indexing. Dalam implementasinya index system dirancang dari penggabungan beberapa cabang ilmu antara lain ilmu bahasa, psikologi, matematika, informatika, fisika, dan ilmu komputer. Tujuan dari penyimpanan data berupa indeks adalah untuk performansi dan kecepatan dalam menemukan informasi yang relevan berdasarkan inputan user. Tanpa adanya indeks, search engine harus melakukan scan terhadap setiap dokumen yang ada didalam database. Hal ini tentu saja akan membutuhkan proses sumber daya yang sangat besar dalam proses komputasi. Sebagai contoh, indeks dari 10.000 dokumen dapat diproses dalam waktu beberapa detik saja, sedangkan penulusuran secara berurutan setiap kata yang terdapat di dalam 10.000 dokumen akan membutuhkan waktu yang berjam lamanya. Tempat tambahan mungkin akan dibutuhkan di dalam computer untuk penyimpanan indeks, tapi hal ini akan terbayar dengan penghematan waktu pada saat pemrosesan pencarian dokumen yang dibutuhkan.","title":"Cara Kerja Web Crawler"},{"location":"carakerja_web_crawler/#cara-kerja-web-crawler","text":"Web crawler atau yang dikenal juga dengan istilah web spider bertugas untuk mengumpulkan semua informasi yang ada di dalam halaman web. Web crawler bekerja secara otomatis dengan cara memberikan sejumlah alamat website untuk dikunjungi serta menyimpan semua informasi yang terkandung didalamnya. Setiap kali web crawler mengunjungi sebuah website, maka web crawler tersebut akan mendata semua link yang ada dihalaman yang dikunjunginya itu untuk kemudian di kunjungi lagi satu persatu. Proses web crawler dalam mengunjungi setiap dokumen web disebut dengan web crawling atau spidering. Beberapa websites, khususnya yang berhubungan dengan pencarian menggunakan proses spidering untuk memperbaharui data data mereka. Web crawler biasa digunakan untuk membuat salinan secara sebagian atau keseluruhan halaman web yang telah dikunjunginya agar dapat di proses lebih lanjut oleh system pengindexan. Crawler dapat juga digunakan untuk proses pemeliharaan sebuah website, seperti memvalidasi kode html sebuah web, dan crawler juga digunakan untuk memperoleh data yang khusus seperti mengumpulkan alamat e-mail. Web crawler termasuk kedalam bagian software agent atau yang lebih dikenal dengan istilah program bot. Secara umum crawler memulai prosesnya dengan memberikan daftar sejumlah alamat website untuk dikunjungi, disebut sebagai seeds. Setiap kali sebuah halaman web dikunjungi, crawler akan mencari alamat yang lain yang terdapat didalamnya dan menambahkan kedalam daftar seeds sebelumnya. Secara umum crawler memulai prosesnya dengan memberikan daftar sejumlah alamat website untuk dikunjungi, disebut sebagai seeds. Setiap kali sebuah halaman web dikunjungi, crawler akan mencari alamat yang lain yang terdapat didalamnya dan menambahkan kedalam daftar seeds sebelumnya. Dalam melakukan prosesnya, web crawler juga mempunyai beberapa persoalan yang harus mampu di atasinya. Permasalahan tersebut mencakup : Halaman mana yang harus dikunjungi terlebih dahulu. Aturan dalam proses mengunjungi kembali sebuah halaman. Performansi, mencakup banyaknya halaman yang harus dikunjungi. Aturan dalam setiap kunjungan agar server yang dikunjungi tidak kelebihan beban. Kegagalan, mencakup tidak tersedianya halaman yang dikunjungi, server down, timeout, maupun jebakan yang sengaja dibuat oleh webmaster. Seberapa jauh kedalaman sebuah website yang akan dikunjungi. Hal yang tak kalah pentingnya adalah kemampuan web crawler untuk mengikuti Perkembangan teknologi web, dimana setiap kali teknologi baru muncul, web crawler harus dapat menyesuaikan diri agar dapat mengunjungi halaman web yang menggunakan teknologi baru tersebut. Proses sebuah web crawler untuk mendata link \u2013 link yang terdapat didalam sebuah halaman web menggunakan pendekatan regular expression. Crawler akan menelurusi setiap karakter yang ada untuk menemukan hyperlink tag html. Setiap hyperlink tag yang ditemukan diperiksa lebih lanjut apakah tag tersebut mengandung atribut nofollow rel, jika tidak ada maka diambil nilai yang terdapat didalam attribute href yang merupakan sebuah link baru. Setelah proses crawler selesai di lanjutkan dengan indexing system yang bertugas untuk menganalisa halaman web yang telah tersimpan sebelumnya dengan cara mengindeks setiap kemungkinan term yang terdapat di dalamnnya. Data term yang ditemukan disimpan dalam sebuah database indeks untuk digunakan dalam pencarian selanjutnya. Indexing system mengumpulkan, memilah dan menyimpan data untuk memberikan kemudahan dalam pengaksesan informasi secara tepat dan akurat. Proses pengolahan halaman web agar dapat digunakan untuk proses pencarian berikutnya dinakamakan web indexing. Dalam implementasinya index system dirancang dari penggabungan beberapa cabang ilmu antara lain ilmu bahasa, psikologi, matematika, informatika, fisika, dan ilmu komputer. Tujuan dari penyimpanan data berupa indeks adalah untuk performansi dan kecepatan dalam menemukan informasi yang relevan berdasarkan inputan user. Tanpa adanya indeks, search engine harus melakukan scan terhadap setiap dokumen yang ada didalam database. Hal ini tentu saja akan membutuhkan proses sumber daya yang sangat besar dalam proses komputasi. Sebagai contoh, indeks dari 10.000 dokumen dapat diproses dalam waktu beberapa detik saja, sedangkan penulusuran secara berurutan setiap kata yang terdapat di dalam 10.000 dokumen akan membutuhkan waktu yang berjam lamanya. Tempat tambahan mungkin akan dibutuhkan di dalam computer untuk penyimpanan indeks, tapi hal ini akan terbayar dengan penghematan waktu pada saat pemrosesan pencarian dokumen yang dibutuhkan.","title":"Cara Kerja Web Crawler"},{"location":"teknik_preprocessing/","text":"Teknik Preprocessing \u00b6 Apa Saja Teknik Data Preprocessing ? Teknik Data Preprocessing : Data Cleaning Data integration Data Reduction Data Transformation Data Cleaning Proses untuk membersihkan data dengan beberapa teknik, yaitu : Memperkecil noise Membetulkan data yang tidak konsisten Mengisi missing value Mengidentifikasi atau membuang outlier Data Cleaning : Missing Values Mengabaikan record Biasanya untuk label klasifikasi yang kosong Mengisikan secara manual Menggunakan mean/median dari atribut yang mengandung missing value Mean dapat dipakai jika distribusi data normal Median digunakan jika distribusi data tidak normal ( condong ) Menggunakan nilai global Menggunakan nilai termungkin Menerapkan regresi Data Cleaning : Noisy Data Noise data adalah suatu kesalahan acak atau variasi dalam variabel terukur Teknik-teknik Binning Smoothing by bin means Smoothing by bin medians Smoothing by bin boundaries Regression Outlier Analysis Data Integration Data dapat bersumber dari beberapa sumber Teknik Analisis korelasi Atribut redudan Duplikasi Data Transformation Tujuannya diharapkan lebih efisien dalam proses data mining dan mungkin juga agar pola yang dihasilkan lebih mudah dipahami Strategi Smoothing Attribute (feature) construction Aggregation Normalization Discretization","title":"Teknik Preprocessing"},{"location":"teknik_preprocessing/#teknik-preprocessing","text":"Apa Saja Teknik Data Preprocessing ? Teknik Data Preprocessing : Data Cleaning Data integration Data Reduction Data Transformation Data Cleaning Proses untuk membersihkan data dengan beberapa teknik, yaitu : Memperkecil noise Membetulkan data yang tidak konsisten Mengisi missing value Mengidentifikasi atau membuang outlier Data Cleaning : Missing Values Mengabaikan record Biasanya untuk label klasifikasi yang kosong Mengisikan secara manual Menggunakan mean/median dari atribut yang mengandung missing value Mean dapat dipakai jika distribusi data normal Median digunakan jika distribusi data tidak normal ( condong ) Menggunakan nilai global Menggunakan nilai termungkin Menerapkan regresi Data Cleaning : Noisy Data Noise data adalah suatu kesalahan acak atau variasi dalam variabel terukur Teknik-teknik Binning Smoothing by bin means Smoothing by bin medians Smoothing by bin boundaries Regression Outlier Analysis Data Integration Data dapat bersumber dari beberapa sumber Teknik Analisis korelasi Atribut redudan Duplikasi Data Transformation Tujuannya diharapkan lebih efisien dalam proses data mining dan mungkin juga agar pola yang dihasilkan lebih mudah dipahami Strategi Smoothing Attribute (feature) construction Aggregation Normalization Discretization","title":"Teknik Preprocessing"}]}